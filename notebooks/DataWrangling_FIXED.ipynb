{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "overview",
   "metadata": {},
   "source": [
    "# üìä Data Wrangling & Cleaning Notebook\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook takes raw student education data and transforms it into analysis-ready datasets. We're working with five interconnected datasets containing information about 307 students across 7 different courses, including their demographics, academic performance, and survey feedback.\n",
    "\n",
    "The raw data has several quality issues we'll address: inconsistent formatting (10 different ways to write \"Semester 1\"!), missing values, duplicate records, and information spread across multiple nationality columns. By the end of this notebook, we'll have clean, validated datasets ready for visualization and analysis.\n",
    "\n",
    "**Our Datasets:**\n",
    "- **Course Codes** (7 courses): Maps course codes to course names\n",
    "- **Student Profiles** (307 students): Demographics, enrollment dates, qualifications\n",
    "- **Student Results** (555 records): GPA and attendance by semester\n",
    "- **Student Survey** (543 records): Student feedback on various factors\n",
    "- **Meta Data** (40 records): Links students to their courses\n",
    "\n",
    "**What We'll Accomplish:**\n",
    "1. Explore and understand the raw data structure\n",
    "2. Standardize formats and fix inconsistencies\n",
    "3. Handle missing values intelligently\n",
    "4. Create useful derived features (age, nationality status, class groupings)\n",
    "5. Merge everything into a master dataset\n",
    "6. Export clean data with full documentation\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09bbc0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# DAVI CA2: Educational Dataset - Complete Data Wrangling Notebook\n",
    "# ===================================================================\n",
    "# Authors: [Student Name 1] + [Student Name 2]\n",
    "# Date: January 2026\n",
    "# Purpose: Clean and prepare educational datasets for analysis\n",
    "# ==================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc_1",
   "metadata": {},
   "source": [
    "## üì¶ CELL 1: Setting Up Our Tools\n",
    "\n",
    "Before we dive into the data, we need to import the Python libraries that will power our analysis. Pandas is our workhorse for data manipulation - think of it as Excel on steroids. NumPy handles numerical operations efficiently. We're also suppressing warning messages to keep our output clean and focused on what matters.\n",
    "\n",
    "The display options we set here will make our data previews more readable throughout the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "624f151e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully\n",
      "Pandas version: 2.2.2\n",
      "NumPy version: 1.26.4\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# CELL 1: Import Required Libraries\n",
    "# ===================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For data visualization during cleaning\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc_2",
   "metadata": {},
   "source": [
    "## üìÇ CELL 2: Loading All Five Datasets\n",
    "\n",
    "Time to bring in our raw data! We're loading five CSV files and immediately checking their dimensions to make sure everything loaded correctly. \n",
    "\n",
    "Looking at the output, we can see:\n",
    "- Course Codes is small (just 7 courses) - this is our reference table\n",
    "- Student Profiles has 307 rows and 15 columns - our main demographic data\n",
    "- Student Results has 555 rows - more rows than students means multiple semesters per student\n",
    "- Student Survey has 543 rows - again, multiple responses per student\n",
    "- Meta Data has 40 rows linking students to courses\n",
    "\n",
    "This size check helps us spot any issues early - like if a file was corrupted or truncated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b99c41e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "LOADING ALL DATASETS\n",
      "======================================================================\n",
      "‚úÖ All datasets loaded successfully\n",
      "\n",
      "Course Codes        :    7 rows √ó  2 columns\n",
      "Meta Data           :   40 rows √ó  3 columns\n",
      "Student Profiles    :  307 rows √ó 15 columns\n",
      "Student Results     :  555 rows √ó  4 columns\n",
      "Student Survey      :  543 rows √ó  8 columns\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# CELL 2: Load All Datasets\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOADING ALL DATASETS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load datasets\n",
    "try:\n",
    "    df_course_codes = pd.read_csv('CA2 datasets & meta data/Course Codes.csv', encoding='utf-8')\n",
    "    df_meta = pd.read_csv('CA2 datasets & meta data/Meta Data.csv', encoding='utf-8')\n",
    "    df_profiles = pd.read_csv('CA2 datasets & meta data/Student Profiles.csv', encoding='utf-8')\n",
    "    df_results = pd.read_csv('CA2 datasets & meta data/Student Results.csv', encoding='utf-8')\n",
    "    df_survey = pd.read_csv('CA2 datasets & meta data/Student Survey.csv', encoding='utf-8')\n",
    "    \n",
    "    print(\"‚úÖ All datasets loaded successfully\\n\")\n",
    "    \n",
    "    # Display shapes\n",
    "    datasets = {\n",
    "        'Course Codes': df_course_codes,\n",
    "        'Meta Data': df_meta,\n",
    "        'Student Profiles': df_profiles,\n",
    "        'Student Results': df_results,\n",
    "        'Student Survey': df_survey\n",
    "    }\n",
    "    \n",
    "    for name, df in datasets.items():\n",
    "        print(f\"{name:20s}: {df.shape[0]:4d} rows √ó {df.shape[1]:2d} columns\")\n",
    "        \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"Please ensure all CSV files are in the same directory as this notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc_3",
   "metadata": {},
   "source": [
    "## üîç CELL 3: Understanding Our Course Catalog\n",
    "\n",
    "Let's start with the simplest dataset - our course codes. This is a lookup table that tells us what each course code means (e.g., 1101 = \"Diploma in Data Analytics with AI\").\n",
    "\n",
    "Good news from the output: no missing values, clean data types. We have 7 courses ranging from diplomas to specialist diplomas to certificates. This small table will be crucial when we want to analyze results by course type or add course names to our reports.\n",
    "\n",
    "Notice the course codes follow a pattern: 1XXX for diplomas, 2XXX for certificates, 5XXX for specialist diplomas. This structure might be useful for grouping courses later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33a5ef04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DATASET 1: COURSE CODES\n",
      "======================================================================\n",
      "\n",
      "First 5 rows:\n",
      "   CODE                              COURSE NAME\n",
      "0  1101        Diploma in Data Analytics with AI\n",
      "1  1102           Diploma in Business Management\n",
      "2  2101         Certificate in Emerging Business\n",
      "3  2102         Certificate in Talent Management\n",
      "4  2013        Certificate in Data Visualization\n",
      "5  5112          Specialist Diploma in eBusiness\n",
      "6  5113  Specialist Diploma in Corporate Finance\n",
      "\n",
      "Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7 entries, 0 to 6\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   CODE         7 non-null      int64 \n",
      " 1   COURSE NAME  7 non-null      object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 244.0+ bytes\n",
      "None\n",
      "\n",
      "Missing Values:\n",
      "CODE           0\n",
      "COURSE NAME    0\n",
      "dtype: int64\n",
      "\n",
      "Unique Courses:\n",
      "['Diploma in Data Analytics with AI', 'Diploma in Business Management', 'Certificate in Emerging Business', 'Certificate in Talent Management', 'Certificate in Data Visualization', 'Specialist Diploma in eBusiness', 'Specialist Diploma in Corporate Finance']\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# CELL 3: Initial Data Exploration - Course Codes\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATASET 1: COURSE CODES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df_course_codes.head(10))\n",
    "\n",
    "print(\"\\nData Info:\")\n",
    "print(df_course_codes.info())\n",
    "\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df_course_codes.isnull().sum())\n",
    "\n",
    "print(\"\\nUnique Courses:\")\n",
    "print(df_course_codes['COURSE NAME'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc_4",
   "metadata": {},
   "source": [
    "## üîç CELL 4: Examining Student Demographics and Enrollment Data\n",
    "\n",
    "Now we're getting into the meat of our data - 307 students with 15 different attributes each. The output reveals some interesting patterns and challenges:\n",
    "\n",
    "**The Good:**\n",
    "- Every student has an ID, gender, date of birth, and enrollment information\n",
    "- Student IDs follow a consistent format: XXXX-CCC/III (course-class/individual)\n",
    "\n",
    "**The Challenges:**\n",
    "- Nationality data is fragmented across 3 columns (SG CITIZEN, SG PR, FOREIGNER) with 89% and 88% missing values - this looks bad but it's actually just poor data structure, not truly missing data\n",
    "- 49% missing \"Country of Other Nationality\" - expected since it only applies to non-citizens\n",
    "- About 6-7% missing course start and end dates - we'll need to handle these carefully\n",
    "\n",
    "The student ID format is particularly interesting - we can extract the class code (the CCC part) to group students who studied together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a31f2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DATASET 2: STUDENT PROFILES\n",
      "======================================================================\n",
      "\n",
      "First 5 rows:\n",
      "     STUDENT ID GENDER SG CITIZEN SG PR FOREIGNER  \\\n",
      "0  1101-009/001      F              NaN         Y   \n",
      "1  1101-009/002      F          Y   NaN       NaN   \n",
      "2  1101-009/003      F              NaN         Y   \n",
      "3  1101-009/004      F              NaN         Y   \n",
      "4  1101-009/005      F          Y   NaN       NaN   \n",
      "\n",
      "  COUNTRY OF OTHER NATIONALITY         DOB HIGHEST QUALIFICATION  \\\n",
      "0                     Malaysia  13/09/1981           Certificate   \n",
      "1                          NaN  26/07/1979           Certificate   \n",
      "2                        India  01/02/1990                Degree   \n",
      "3                  Netherlands  20/04/1976               Diploma   \n",
      "4                          NaN  25/11/1983               Diploma   \n",
      "\n",
      "               NAME OF QUALIFICATION AND INSTITUTION  \\\n",
      "0                                                SPM   \n",
      "1                  Certificate in Office Skills, ITE   \n",
      "2  Bachelor of Business Administration, Universit...   \n",
      "3  Office Management Diploma, NCOI Rotterdam, The...   \n",
      "4  Diploma in Business Admininstration, LCCI Leve...   \n",
      "\n",
      "  DATE ATTAINED HIGHEST QUALIFICATION                  DESIGNATION  \\\n",
      "0                          08/01/2018         Admin & HR Assistant   \n",
      "1                          08/06/2016              Admin Assistant   \n",
      "2                          08/08/2015                            -   \n",
      "3                          08/02/2018  HR Support / Office Manager   \n",
      "4                          08/06/2015    Executive, Administration   \n",
      "\n",
      "  COMMENCEMENT DATE COMPLETION DATE FULL-TIME OR PART-TIME  COURSE FUNDING  \n",
      "0        18/04/2022      17/09/2023              Part-Time      Individual  \n",
      "1        18/04/2022      17/09/2023              Part-Time  Individual-SFC  \n",
      "2        18/04/2022      17/09/2023              Part-Time      Individual  \n",
      "3        18/04/2022      17/09/2023              Part-Time      Individual  \n",
      "4        18/04/2022      17/09/2023              Part-Time       Sponsored  \n",
      "\n",
      "Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 307 entries, 0 to 306\n",
      "Data columns (total 15 columns):\n",
      " #   Column                                 Non-Null Count  Dtype \n",
      "---  ------                                 --------------  ----- \n",
      " 0   STUDENT ID                             307 non-null    object\n",
      " 1   GENDER                                 307 non-null    object\n",
      " 2   SG CITIZEN                             268 non-null    object\n",
      " 3   SG PR                                  32 non-null     object\n",
      " 4   FOREIGNER                              36 non-null     object\n",
      " 5   COUNTRY OF OTHER NATIONALITY           156 non-null    object\n",
      " 6   DOB                                    307 non-null    object\n",
      " 7   HIGHEST QUALIFICATION                  307 non-null    object\n",
      " 8   NAME OF QUALIFICATION AND INSTITUTION  307 non-null    object\n",
      " 9   DATE ATTAINED HIGHEST QUALIFICATION    307 non-null    object\n",
      " 10  DESIGNATION                            307 non-null    object\n",
      " 11  COMMENCEMENT DATE                      302 non-null    object\n",
      " 12  COMPLETION DATE                        287 non-null    object\n",
      " 13  FULL-TIME OR PART-TIME                 307 non-null    object\n",
      " 14  COURSE FUNDING                         307 non-null    object\n",
      "dtypes: object(15)\n",
      "memory usage: 36.1+ KB\n",
      "None\n",
      "\n",
      "Missing Values Summary:\n",
      "                              Missing Count  Percentage\n",
      "SG PR                                   275       89.58\n",
      "FOREIGNER                               271       88.27\n",
      "COUNTRY OF OTHER NATIONALITY            151       49.19\n",
      "SG CITIZEN                               39       12.70\n",
      "COMPLETION DATE                          20        6.51\n",
      "COMMENCEMENT DATE                         5        1.63\n",
      "\n",
      "Column Names:\n",
      " 1. STUDENT ID\n",
      " 2. GENDER\n",
      " 3. SG CITIZEN\n",
      " 4. SG PR\n",
      " 5. FOREIGNER\n",
      " 6. COUNTRY OF OTHER NATIONALITY\n",
      " 7. DOB\n",
      " 8. HIGHEST QUALIFICATION\n",
      " 9. NAME OF QUALIFICATION AND INSTITUTION\n",
      "10. DATE ATTAINED HIGHEST QUALIFICATION\n",
      "11. DESIGNATION\n",
      "12. COMMENCEMENT DATE\n",
      "13. COMPLETION DATE\n",
      "14. FULL-TIME OR PART-TIME\n",
      "15. COURSE FUNDING\n",
      "\n",
      "Sample STUDENT IDs (to understand format):\n",
      "['1101-009/001', '1101-009/002', '1101-009/003', '1101-009/004', '1101-009/005', '1101-009/006', '1101-009/007', '1101-009/008', '1101-009/009', '1101-009/010']\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# CELL 4: Initial Data Exploration - Student Profiles\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATASET 2: STUDENT PROFILES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df_profiles.head())\n",
    "\n",
    "print(\"\\nData Info:\")\n",
    "print(df_profiles.info())\n",
    "\n",
    "print(\"\\nMissing Values Summary:\")\n",
    "missing_profiles = df_profiles.isnull().sum()\n",
    "missing_pct = (missing_profiles / len(df_profiles) * 100).round(2)\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_profiles,\n",
    "    'Percentage': missing_pct\n",
    "}).sort_values('Percentage', ascending=False)\n",
    "print(missing_df[missing_df['Missing Count'] > 0])\n",
    "\n",
    "print(\"\\nColumn Names:\")\n",
    "for i, col in enumerate(df_profiles.columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")\n",
    "\n",
    "# Sample STUDENT IDs to understand format\n",
    "print(\"\\nSample STUDENT IDs (to understand format):\")\n",
    "print(df_profiles['STUDENT ID'].head(10).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc_5",
   "metadata": {},
   "source": [
    "## üîç CELL 5: Analyzing Academic Performance Data\n",
    "\n",
    "The Student Results dataset tracks GPA and attendance across semesters. With 555 rows for 291 unique students, most students have 2-3 semesters of data.\n",
    "\n",
    "**Key Findings:**\n",
    "- GPA ranges from 1.6 to 4.0, with an average of 3.1 - reasonable for this grading scale\n",
    "- Attendance averages 86%, ranging from 50% to 100%\n",
    "- No missing values in GPA or attendance - great!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bc74ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DATASET 3: STUDENT RESULTS\n",
      "======================================================================\n",
      "\n",
      "First 10 rows:\n",
      "     STUDENT ID PERIOD  GPA  ATTENDANCE\n",
      "0  1101-009/001  Sem 1  3.5         100\n",
      "1  1101-009/001  Sem 2  3.6         100\n",
      "2  1101-009/001  Sem 3  3.7          80\n",
      "3  1101-009/002  Sem 1  3.4         100\n",
      "4  1101-009/002  Sem 2  3.5          80\n",
      "5  1101-009/002  Sem 3  3.6          97\n",
      "6  1101-009/003  Sem 1  3.3         100\n",
      "7  1101-009/003  Sem 2  3.2         100\n",
      "8  1101-009/003  Sem 3  3.6          91\n",
      "9  1101-009/004  Sem 1  3.9         100\n",
      "\n",
      "Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 555 entries, 0 to 554\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   STUDENT ID  555 non-null    object \n",
      " 1   PERIOD      555 non-null    object \n",
      " 2   GPA         555 non-null    float64\n",
      " 3   ATTENDANCE  555 non-null    int64  \n",
      "dtypes: float64(1), int64(1), object(2)\n",
      "memory usage: 17.5+ KB\n",
      "None\n",
      "\n",
      "Missing Values:\n",
      "STUDENT ID    0\n",
      "PERIOD        0\n",
      "GPA           0\n",
      "ATTENDANCE    0\n",
      "dtype: int64\n",
      "\n",
      "Basic Statistics:\n",
      "              GPA  ATTENDANCE\n",
      "count  555.000000  555.000000\n",
      "mean     3.107568   85.821622\n",
      "std      0.601875   12.561570\n",
      "min      1.600000   50.000000\n",
      "25%      2.700000   80.000000\n",
      "50%      3.200000   87.000000\n",
      "75%      3.600000   97.500000\n",
      "max      4.000000  100.000000\n",
      "\n",
      "Unique values per column:\n",
      "STUDENT ID     :  291 unique values\n",
      "PERIOD         :   10 unique values\n",
      "GPA            :   25 unique values\n",
      "ATTENDANCE     :   49 unique values\n",
      "\n",
      "PERIOD values:\n",
      "PERIOD\n",
      "Sem 1         223\n",
      "Sem 2         124\n",
      "Sem 3          45\n",
      "Sem 4           1\n",
      "Sem1           30\n",
      "Sem2            1\n",
      "Semester 1     56\n",
      "Semester 2     37\n",
      "Semester 3     37\n",
      "Semester 4      1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# CELL 5: Initial Data Exploration - Student Results\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATASET 3: STUDENT RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nFirst 10 rows:\")\n",
    "print(df_results.head(10))\n",
    "\n",
    "print(\"\\nData Info:\")\n",
    "print(df_results.info())\n",
    "\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df_results.isnull().sum())\n",
    "\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(df_results.describe())\n",
    "\n",
    "print(\"\\nUnique values per column:\")\n",
    "for col in df_results.columns:\n",
    "    print(f\"{col:15s}: {df_results[col].nunique():4d} unique values\")\n",
    "\n",
    "print(\"\\nPERIOD values:\")\n",
    "print(df_results['PERIOD'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc_6",
   "metadata": {},
   "source": [
    "## üîç CELL 6: Reviewing Student Feedback and Survey Responses\n",
    "\n",
    "The survey dataset captures student perceptions across 6 dimensions: prior knowledge, course relevance, teaching support, company support, family support, and weekly self-study hours.\n",
    "\n",
    "Looking at the data structure, survey responses use a 1-5 Likert scale (1=lowest, 5=highest) for the support and relevance questions. Self-study hours are reported as actual hours per week. \n",
    "\n",
    "**Same Issues as Results:**\n",
    "- The PERIOD column has identical formatting problems (10 different formats)\n",
    "- 32 duplicate student-semester records need removal\n",
    "\n",
    "The good news? No missing survey responses - every student who participated completed all questions. This suggests the survey had good quality controls during data collection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e357da2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DATASET 4: STUDENT SURVEY\n",
      "======================================================================\n",
      "\n",
      "First 10 rows:\n",
      "     STUDENT ID PERIOD  PRIOR KNOWLEDGE  COURSE RELEVANCE  TEACHING SUPPORT  \\\n",
      "0  1101-009/001  Sem 1                4                 3                 4   \n",
      "1  1101-009/001  Sem 2                4                 5                 5   \n",
      "2  1101-009/001  Sem 3                5                 4                 4   \n",
      "3  1101-009/002  Sem 1                2                 4                 5   \n",
      "4  1101-009/002  Sem 2                3                 5                 5   \n",
      "5  1101-009/002  Sem 3                3                 5                 5   \n",
      "6  1101-009/003  Sem 1                4                 4                 3   \n",
      "7  1101-009/003  Sem 2                4                 5                 4   \n",
      "8  1101-009/003  Sem 3                5                 4                 5   \n",
      "9  1101-009/004  Sem 1                4                 4                 3   \n",
      "\n",
      "   COMPANY SUPPORT  FAMILY SUPPORT  SELF-STUDY HRS  \n",
      "0                5               3              16  \n",
      "1                3               3              14  \n",
      "2                4               3              18  \n",
      "3                3               3              14  \n",
      "4                5               4              17  \n",
      "5                3               5              13  \n",
      "6                4               5              15  \n",
      "7                3               3              20  \n",
      "8                3               3              14  \n",
      "9                3               3              18  \n",
      "\n",
      "Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 543 entries, 0 to 542\n",
      "Data columns (total 8 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   STUDENT ID        543 non-null    object\n",
      " 1   PERIOD            543 non-null    object\n",
      " 2   PRIOR KNOWLEDGE   543 non-null    int64 \n",
      " 3   COURSE RELEVANCE  543 non-null    int64 \n",
      " 4   TEACHING SUPPORT  543 non-null    int64 \n",
      " 5   COMPANY SUPPORT   543 non-null    int64 \n",
      " 6   FAMILY SUPPORT    543 non-null    int64 \n",
      " 7   SELF-STUDY HRS    543 non-null    int64 \n",
      "dtypes: int64(6), object(2)\n",
      "memory usage: 34.1+ KB\n",
      "None\n",
      "\n",
      "Missing Values:\n",
      "STUDENT ID          0\n",
      "PERIOD              0\n",
      "PRIOR KNOWLEDGE     0\n",
      "COURSE RELEVANCE    0\n",
      "TEACHING SUPPORT    0\n",
      "COMPANY SUPPORT     0\n",
      "FAMILY SUPPORT      0\n",
      "SELF-STUDY HRS      0\n",
      "dtype: int64\n",
      "\n",
      "Basic Statistics:\n",
      "       PRIOR KNOWLEDGE  COURSE RELEVANCE  TEACHING SUPPORT  COMPANY SUPPORT  \\\n",
      "count       543.000000        543.000000        543.000000       543.000000   \n",
      "mean          3.686924          3.939227          3.712707         3.858195   \n",
      "std           0.862051          0.902022          1.049673         0.977686   \n",
      "min           1.000000          1.000000          1.000000         1.000000   \n",
      "25%           3.000000          3.000000          3.000000         3.000000   \n",
      "50%           4.000000          4.000000          4.000000         4.000000   \n",
      "75%           4.000000          5.000000          5.000000         5.000000   \n",
      "max           5.000000          5.000000          5.000000         5.000000   \n",
      "\n",
      "       FAMILY SUPPORT  SELF-STUDY HRS  \n",
      "count       543.00000      543.000000  \n",
      "mean          3.85267       13.384899  \n",
      "std           0.95683        4.218776  \n",
      "min           1.00000        5.000000  \n",
      "25%           3.00000       10.000000  \n",
      "50%           4.00000       13.000000  \n",
      "75%           5.00000       17.000000  \n",
      "max           5.00000       23.000000  \n",
      "\n",
      "Unique values per column:\n",
      "STUDENT ID          :  285 unique values\n",
      "PERIOD              :   10 unique values\n",
      "PRIOR KNOWLEDGE     :    5 unique values\n",
      "COURSE RELEVANCE    :    5 unique values\n",
      "TEACHING SUPPORT    :    5 unique values\n",
      "COMPANY SUPPORT     :    5 unique values\n",
      "FAMILY SUPPORT      :    5 unique values\n",
      "SELF-STUDY HRS      :   19 unique values\n",
      "\n",
      "PERIOD values:\n",
      "PERIOD\n",
      "Sem 1         238\n",
      "Sem 2         123\n",
      "Sem 3          43\n",
      "Sem 4           1\n",
      "Sem1           29\n",
      "Sem2            1\n",
      "Semester 1     36\n",
      "Semester 2     36\n",
      "Semester 3     35\n",
      "Semester 4      1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# CELL 6: Initial Data Exploration - Student Survey\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATASET 4: STUDENT SURVEY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nFirst 10 rows:\")\n",
    "print(df_survey.head(10))\n",
    "\n",
    "print(\"\\nData Info:\")\n",
    "print(df_survey.info())\n",
    "\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df_survey.isnull().sum())\n",
    "\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(df_survey.describe())\n",
    "\n",
    "print(\"\\nUnique values per column:\")\n",
    "for col in df_survey.columns:\n",
    "    print(f\"{col:20s}: {df_survey[col].nunique():4d} unique values\")\n",
    "\n",
    "print(\"\\nPERIOD values:\")\n",
    "print(df_survey['PERIOD'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc_7",
   "metadata": {},
   "source": [
    "## üîç CELL 7: Mapping Students to Courses\n",
    "\n",
    "The metadata serves as a bridge between students and courses. With only 40 rows linking to our 307 students, this tells us that not every student has a course mapping in this file - probably because this table only tracks certain cohorts or was extracted from a larger database.\n",
    "\n",
    "This small linking table has just three columns: STUDENT ID, CODE (course code), and COURSE NAME. We'll use this to enrich our analysis by adding course context to student performance data.\n",
    "\n",
    "The course names here should match our Course Codes reference table - we'll verify this during cleaning to ensure consistency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbdb518a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PARSING STUDENT ID - EXTRACTING CLASS CODE\n",
      "======================================================================\n",
      "\n",
      "Extracting CLASS codes from STUDENT ID...\n",
      "‚úÖ Profiles: Extracted CLASS for 307 students\n",
      "‚úÖ Results: Extracted CLASS for 555 records\n",
      "‚úÖ Survey: Extracted CLASS for 543 records\n",
      "\n",
      "üìä Unique CLASS codes found:\n",
      "Total unique classes: 32\n",
      "Classes: ['1101-009', '1101-010', '1101-011', '1101-012', '1102-001', '1102-002', '1102-003', '1102-004', '2101-106', '2101-107', '2101-108A', '2101-109', '2101-110', '2101-111', '2102-063', '2102-064', '2102-065A', '2102-066', '2102-067A', '2102-068A', '2102-069', '2102-070', '5112-007', '5112-008', '5112-009', '5112-010', '5112-011', '5113-005', '5113-006', '5113-007', '5113-008', '5113-009']\n",
      "\n",
      "CLASS distribution in Profiles:\n",
      "CLASS\n",
      "1101-009     11\n",
      "1101-010     12\n",
      "1101-011     10\n",
      "1101-012     11\n",
      "1102-001      8\n",
      "1102-002      7\n",
      "1102-003      7\n",
      "1102-004      8\n",
      "2101-107     10\n",
      "2101-108A     8\n",
      "2101-109      8\n",
      "2101-110      8\n",
      "2101-111      8\n",
      "2102-063     16\n",
      "2102-064     14\n",
      "2102-065A    12\n",
      "2102-066     16\n",
      "2102-067A    11\n",
      "2102-068A    10\n",
      "2102-069     11\n",
      "2102-070     11\n",
      "5112-008     14\n",
      "5112-009     11\n",
      "5112-010     10\n",
      "5112-011     10\n",
      "5113-005      7\n",
      "5113-006      7\n",
      "5113-007     17\n",
      "5113-008      7\n",
      "5113-009      7\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# CELL 7: Parse STUDENT ID to Extract CLASS Code\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PARSING STUDENT ID - EXTRACTING CLASS CODE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# STUDENT ID Format: XXXX-CCC/III\n",
    "# XXXX = 4 digit number\n",
    "# CCC = 3 digit CLASS code (this is what we need!)\n",
    "# III = 3 digit student index within class\n",
    "\n",
    "def extract_class_from_student_id(student_id):\n",
    "    \"\"\"\n",
    "    Extract the CLASS code from STUDENT ID\n",
    "    Format: XXXX-CCC/III \n",
    "    We need XXXX-CCC (first 4 digits + dash + 3 digit class code)\n",
    "    Example: 1101-009/002 ‚Üí class code is '1101-009'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if pd.isna(student_id):\n",
    "            return None\n",
    "        student_id = str(student_id).strip()\n",
    "        \n",
    "        # Split by '/' to remove the student index\n",
    "        if '/' in student_id:\n",
    "            class_code = student_id.split('/')[0]  # Gets '1101-009'\n",
    "            return class_code\n",
    "        else:\n",
    "            return student_id if '-' in student_id else None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Apply to all datasets that have STUDENT ID\n",
    "print(\"\\nExtracting CLASS codes from STUDENT ID...\")\n",
    "\n",
    "# Profiles\n",
    "df_profiles['CLASS'] = df_profiles['STUDENT ID'].apply(extract_class_from_student_id)\n",
    "print(f\"‚úÖ Profiles: Extracted CLASS for {df_profiles['CLASS'].notna().sum()} students\")\n",
    "\n",
    "# Results  \n",
    "df_results['CLASS'] = df_results['STUDENT ID'].apply(extract_class_from_student_id)\n",
    "print(f\"‚úÖ Results: Extracted CLASS for {df_results['CLASS'].notna().sum()} records\")\n",
    "\n",
    "# Survey\n",
    "df_survey['CLASS'] = df_survey['STUDENT ID'].apply(extract_class_from_student_id)\n",
    "print(f\"‚úÖ Survey: Extracted CLASS for {df_survey['CLASS'].notna().sum()} records\")\n",
    "\n",
    "# Show unique classes\n",
    "print(f\"\\nüìä Unique CLASS codes found:\")\n",
    "all_classes = pd.concat([df_profiles['CLASS'], df_results['CLASS'], df_survey['CLASS']]).unique()\n",
    "all_classes = sorted([c for c in all_classes if pd.notna(c)])\n",
    "print(f\"Total unique classes: {len(all_classes)}\")\n",
    "print(f\"Classes: {all_classes}\")\n",
    "\n",
    "# Show distribution\n",
    "print(f\"\\nCLASS distribution in Profiles:\")\n",
    "print(df_profiles['CLASS'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc_8",
   "metadata": {},
   "source": [
    "## üîç CELL 8: Checking How Our Datasets Connect\n",
    "\n",
    "Before merging datasets, we need to understand how they relate to each other. The output reveals some important mismatches:\n",
    "\n",
    "**Student ID Coverage:**\n",
    "- 295 unique students in Profiles\n",
    "- 291 in Results (11 students have results but no profile!)\n",
    "- 285 in Survey (21 students have profiles but no survey responses)\n",
    "\n",
    "These gaps mean we can't do a simple inner join - we'll need left joins to preserve all student profiles while acknowledging that not everyone has complete data.\n",
    "\n",
    "**Duplicate Issues Confirmed:**\n",
    "- 12 duplicate student IDs in Profiles (same student appearing twice)\n",
    "- 33 duplicates in Results (same student-semester combo)\n",
    "- 32 duplicates in Survey (same student-semester combo)\n",
    "\n",
    "The PERIOD formatting chaos is consistent across Results and Survey - both have the exact same 10 format variations. At least they're consistently inconsistent!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f16d5037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CROSS-DATASET RELATIONSHIP ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Unique Student IDs:\n",
      "  Profiles: 295\n",
      "  Results:  291\n",
      "  Survey:   285\n",
      "\n",
      "Students in Results but NOT in Profiles: 11\n",
      "Students in Survey but NOT in Profiles:  11\n",
      "Students in Profiles but NOT in Results: 15\n",
      "Students in Profiles but NOT in Survey:  21\n",
      "\n",
      "PERIOD Analysis:\n",
      "  Results PERIOD values: ['Sem 1', 'Sem 2', 'Sem 3', 'Sem 4', 'Sem1', 'Sem2', 'Semester 1', 'Semester 2', 'Semester 3', 'Semester 4']\n",
      "  Survey PERIOD values:  ['Sem 1', 'Sem 2', 'Sem 3', 'Sem 4', 'Sem1', 'Sem2', 'Semester 1', 'Semester 2', 'Semester 3', 'Semester 4']\n",
      "\n",
      "Duplicate Analysis:\n",
      "  Profiles duplicates (STUDENT ID): 12\n",
      "  Results duplicates (STUDENT ID + PERIOD): 33\n",
      "  Survey duplicates (STUDENT ID + PERIOD): 32\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# CELL 8: Cross-Dataset Relationship Analysis\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CROSS-DATASET RELATIONSHIP ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check STUDENT ID consistency\n",
    "unique_profiles = set(df_profiles['STUDENT ID'].unique())\n",
    "unique_results = set(df_results['STUDENT ID'].unique())\n",
    "unique_survey = set(df_survey['STUDENT ID'].unique())\n",
    "\n",
    "print(f\"\\nUnique Student IDs:\")\n",
    "print(f\"  Profiles: {len(unique_profiles)}\")\n",
    "print(f\"  Results:  {len(unique_results)}\")\n",
    "print(f\"  Survey:   {len(unique_survey)}\")\n",
    "\n",
    "print(f\"\\nStudents in Results but NOT in Profiles: {len(unique_results - unique_profiles)}\")\n",
    "print(f\"Students in Survey but NOT in Profiles:  {len(unique_survey - unique_profiles)}\")\n",
    "print(f\"Students in Profiles but NOT in Results: {len(unique_profiles - unique_results)}\")\n",
    "print(f\"Students in Profiles but NOT in Survey:  {len(unique_profiles - unique_survey)}\")\n",
    "\n",
    "# Check PERIOD consistency\n",
    "print(f\"\\nPERIOD Analysis:\")\n",
    "print(f\"  Results PERIOD values: {sorted(df_results['PERIOD'].unique())}\")\n",
    "print(f\"  Survey PERIOD values:  {sorted(df_survey['PERIOD'].unique())}\")\n",
    "\n",
    "# Check for duplicates\n",
    "print(f\"\\nDuplicate Analysis:\")\n",
    "print(f\"  Profiles duplicates (STUDENT ID): {df_profiles['STUDENT ID'].duplicated().sum()}\")\n",
    "print(f\"  Results duplicates (STUDENT ID + PERIOD): {df_results.duplicated(subset=['STUDENT ID', 'PERIOD']).sum()}\")\n",
    "print(f\"  Survey duplicates (STUDENT ID + PERIOD): {df_survey.duplicated(subset=['STUDENT ID', 'PERIOD']).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc_9",
   "metadata": {},
   "source": [
    "## üßπ CELL 9: Cleaning the Course Reference Table\n",
    "\n",
    "Starting with our simplest dataset to establish a clean foundation. We're standardizing column names to uppercase (for consistency across all datasets) and stripping any sneaky whitespace that could cause matching problems later.\n",
    "\n",
    "The output shows the cleaned table - all 7 courses with clean formatting. This might seem like overkill for such a small table, but when we merge datasets, even one extra space in \"Diploma in Business Management \" could break the join and cost us hours of debugging.\n",
    "\n",
    "No duplicates found, all values present - this table is good to go!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04f7d5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CLEANING: COURSE CODES\n",
      "======================================================================\n",
      "Before cleaning:\n",
      "  Shape: (7, 2)\n",
      "  Nulls: 0\n",
      "\n",
      "After cleaning:\n",
      "  Shape: (7, 2)\n",
      "   CODE                              COURSE NAME\n",
      "0  1101        Diploma in Data Analytics with AI\n",
      "1  1102           Diploma in Business Management\n",
      "2  2101         Certificate in Emerging Business\n",
      "3  2102         Certificate in Talent Management\n",
      "4  2013        Certificate in Data Visualization\n",
      "5  5112          Specialist Diploma in eBusiness\n",
      "6  5113  Specialist Diploma in Corporate Finance\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# CELL 9: Clean Course Codes Dataset\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLEANING: COURSE CODES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "df_course_codes_clean = df_course_codes.copy()\n",
    "\n",
    "# Check for issues\n",
    "print(\"Before cleaning:\")\n",
    "print(f\"  Shape: {df_course_codes_clean.shape}\")\n",
    "print(f\"  Nulls: {df_course_codes_clean.isnull().sum().sum()}\")\n",
    "\n",
    "# Remove any whitespace from column names\n",
    "df_course_codes_clean.columns = df_course_codes_clean.columns.str.strip()\n",
    "\n",
    "# Remove whitespace from string columns\n",
    "for col in df_course_codes_clean.select_dtypes(include='object').columns:\n",
    "    df_course_codes_clean[col] = df_course_codes_clean[col].str.strip()\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = df_course_codes_clean.duplicated().sum()\n",
    "if duplicates > 0:\n",
    "    print(f\"  ‚ö†Ô∏è  Found {duplicates} duplicate rows - removing...\")\n",
    "    df_course_codes_clean = df_course_codes_clean.drop_duplicates()\n",
    "\n",
    "print(\"\\nAfter cleaning:\")\n",
    "print(f\"  Shape: {df_course_codes_clean.shape}\")\n",
    "print(df_course_codes_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc_10",
   "metadata": {},
   "source": [
    "## üßπ CELL 10: Consolidating Nationality Information\n",
    "\n",
    "Here we're tackling one of our biggest data quality issues - those three nationality columns (SG CITIZEN, SG PR, FOREIGNER) that appear to be 89% and 88% missing. The reality is they're not missing at all - they're just structured poorly.\n",
    "\n",
    "**The Problem:**\n",
    "Each student should have exactly ONE of these flags set to 'Y', but they're stored in three separate columns. Plus, the 'Y' values are inconsistent - some are 'Y', some are 'Yes', some are blank strings.\n",
    "\n",
    "**Our Solution:**\n",
    "We standardize all the yes/no values to 'Y'/'N', then create a single NATIONALITY_STATUS column. Now instead of three confusing columns, we have one clear field.\n",
    "\n",
    "**The Result:**\n",
    "Looking at the distribution in the output:\n",
    "- 239 SG Citizens (77.9%)\n",
    "- 36 Foreigners (11.7%) \n",
    "- 32 SG PRs (10.4%)\n",
    "\n",
    "Much clearer! The validation check confirms no student has multiple flags or zero flags - everyone fits exactly one category.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0ed706a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CLEANING: STUDENT PROFILES - PART 1 (Columns)\n",
      "======================================================================\n",
      "Step 1: Standardize column names\n",
      "  ‚úÖ Standardized 16 column names\n",
      "\n",
      "Step 2: Remove whitespace from all string columns\n",
      "  ‚úÖ Whitespace removed\n",
      "\n",
      "Step 3: Standardize and fix nationality columns\n",
      "\n",
      "üìä BEFORE Standardization:\n",
      "SG CITIZEN unique values: ['' 'Y' nan 'Yes']\n",
      "SG PR unique values: [nan 'Y' 'Yes']\n",
      "FOREIGNER unique values: ['Y' nan]\n",
      "\n",
      "üìä AFTER Standardization:\n",
      "SG CITIZEN unique values: ['N' 'Y']\n",
      "SG PR unique values: ['N' 'Y']\n",
      "FOREIGNER unique values: ['Y' 'N']\n",
      "\n",
      "‚úÖ Created NATIONALITY_STATUS column\n",
      "\n",
      "üìä Distribution:\n",
      "NATIONALITY_STATUS\n",
      "SG Citizen    239\n",
      "Foreigner      36\n",
      "SG PR          32\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Percentages:\n",
      "  SG Citizen: 239 (77.9%)\n",
      "  Foreigner: 36 (11.7%)\n",
      "  SG PR: 32 (10.4%)\n",
      "\n",
      "‚úÖ No Unknown nationality cases!\n",
      "\n",
      "üîç Data Quality Check:\n",
      "  ‚úÖ No multiple flags\n",
      "  ‚úÖ All students have flags\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# CELL 10: Clean Student Profiles - Part 1 (Column Cleanup)\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLEANING: STUDENT PROFILES - PART 1 (Columns)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "df_profiles_clean = df_profiles.copy()\n",
    "\n",
    "print(\"Step 1: Standardize column names\")\n",
    "df_profiles_clean.columns = df_profiles_clean.columns.str.strip().str.upper()\n",
    "print(f\"  ‚úÖ Standardized {len(df_profiles_clean.columns)} column names\")\n",
    "\n",
    "print(\"\\nStep 2: Remove whitespace from all string columns\")\n",
    "for col in df_profiles_clean.select_dtypes(include='object').columns:\n",
    "    if col != 'CLASS':  # Don't strip CLASS since we just created it\n",
    "        df_profiles_clean[col] = df_profiles_clean[col].str.strip()\n",
    "print(\"  ‚úÖ Whitespace removed\")\n",
    "\n",
    "print(\"\\nStep 3: Standardize and fix nationality columns\")\n",
    "\n",
    "# First, standardize the citizenship columns (Y/N instead of Yes/No)\n",
    "print(\"\\nüìä BEFORE Standardization:\")\n",
    "print(f\"SG CITIZEN unique values: {df_profiles_clean['SG CITIZEN'].unique()}\")\n",
    "print(f\"SG PR unique values: {df_profiles_clean['SG PR'].unique()}\")\n",
    "print(f\"FOREIGNER unique values: {df_profiles_clean['FOREIGNER'].unique()}\")\n",
    "\n",
    "def standardize_yes_no(value):\n",
    "    \"\"\"\n",
    "    Standardize Yes/No/Y/N values to consistent 'Y' or 'N'\n",
    "    \"\"\"\n",
    "    if pd.isna(value):\n",
    "        return 'N'\n",
    "    \n",
    "    value_str = str(value).strip().upper()\n",
    "    \n",
    "    if value_str in ['YES', 'Y', '1', 'TRUE']:\n",
    "        return 'Y'\n",
    "    elif value_str in ['NO', 'N', '0', 'FALSE', '']:\n",
    "        return 'N'\n",
    "    else:\n",
    "        return 'N'\n",
    "\n",
    "# Apply standardization\n",
    "df_profiles_clean['SG CITIZEN'] = df_profiles_clean['SG CITIZEN'].apply(standardize_yes_no)\n",
    "df_profiles_clean['SG PR'] = df_profiles_clean['SG PR'].apply(standardize_yes_no)\n",
    "df_profiles_clean['FOREIGNER'] = df_profiles_clean['FOREIGNER'].apply(standardize_yes_no)\n",
    "\n",
    "print(\"\\nüìä AFTER Standardization:\")\n",
    "print(f\"SG CITIZEN unique values: {df_profiles_clean['SG CITIZEN'].unique()}\")\n",
    "print(f\"SG PR unique values: {df_profiles_clean['SG PR'].unique()}\")\n",
    "print(f\"FOREIGNER unique values: {df_profiles_clean['FOREIGNER'].unique()}\")\n",
    "\n",
    "# Now create NATIONALITY_STATUS\n",
    "def determine_nationality_simple(row):\n",
    "    if row['SG CITIZEN'] == 'Y':\n",
    "        return 'SG Citizen'\n",
    "    elif row['SG PR'] == 'Y':\n",
    "        return 'SG PR'\n",
    "    elif row['FOREIGNER'] == 'Y':\n",
    "        return 'Foreigner'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "df_profiles_clean['NATIONALITY_STATUS'] = df_profiles_clean.apply(\n",
    "    determine_nationality_simple, axis=1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Created NATIONALITY_STATUS column\")\n",
    "print(\"\\nüìä Distribution:\")\n",
    "nationality_counts = df_profiles_clean['NATIONALITY_STATUS'].value_counts()\n",
    "print(nationality_counts)\n",
    "\n",
    "print(\"\\nPercentages:\")\n",
    "for status, count in nationality_counts.items():\n",
    "    pct = (count / len(df_profiles_clean) * 100)\n",
    "    print(f\"  {status}: {count} ({pct:.1f}%)\")\n",
    "\n",
    "unknown_cases = df_profiles_clean[df_profiles_clean['NATIONALITY_STATUS'] == 'Unknown']\n",
    "if len(unknown_cases) > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è  WARNING: {len(unknown_cases)} students have 'Unknown' nationality\")\n",
    "    print(\"Sample:\")\n",
    "    print(unknown_cases[['STUDENT ID', 'SG CITIZEN', 'SG PR', 'FOREIGNER']].head(10))\n",
    "else:\n",
    "    print(\"\\n‚úÖ No Unknown nationality cases!\")\n",
    "\n",
    "print(\"\\nüîç Data Quality Check:\")\n",
    "df_profiles_clean['citizenship_flags_count'] = (\n",
    "    (df_profiles_clean['SG CITIZEN'] == 'Y').astype(int) +\n",
    "    (df_profiles_clean['SG PR'] == 'Y').astype(int) +\n",
    "    (df_profiles_clean['FOREIGNER'] == 'Y').astype(int)\n",
    ")\n",
    "\n",
    "multiple_flags = df_profiles_clean[df_profiles_clean['citizenship_flags_count'] > 1]\n",
    "if len(multiple_flags) > 0:\n",
    "    print(f\"  ‚ö†Ô∏è  {len(multiple_flags)} students have multiple citizenship flags\")\n",
    "else:\n",
    "    print(\"  ‚úÖ No multiple flags\")\n",
    "\n",
    "no_flags = df_profiles_clean[df_profiles_clean['citizenship_flags_count'] == 0]\n",
    "if len(no_flags) > 0:\n",
    "    print(f\"  ‚ö†Ô∏è  {len(no_flags)} students have NO citizenship flag\")\n",
    "else:\n",
    "    print(\"  ‚úÖ All students have flags\")\n",
    "\n",
    "df_profiles_clean = df_profiles_clean.drop('citizenship_flags_count', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc_11",
   "metadata": {},
   "source": [
    "## üßπ CELL 11: Converting Dates and Engineering New Features\n",
    "\n",
    "Raw dates stored as text strings like \"13/09/1981\" are useless for analysis. We're converting all date columns to proper datetime format, which unlocks time-based calculations.\n",
    "\n",
    "**New Features Created:**\n",
    "\n",
    "*AGE:* Calculated from date of birth. The output shows ages range from 15 to 65 years old with an average of 41 - these are working adults pursuing further education.\n",
    "\n",
    "*CLASS:* Extracted from student ID format (XXXX-CCC/III). For example, \"1101-009/001\" ‚Üí CLASS is \"009\". This groups students who studied together, which we'll use for imputing missing dates.\n",
    "\n",
    "*COURSE_DURATION_DAYS:* Calculated as completion date minus commencement date. The output shows an average of 278 days (about 9 months) with a wide range from 30 to 700 days - some students finish quickly, others take much longer.\n",
    "\n",
    "**Data Quality Notes:**\n",
    "The conversion reveals 4 missing DOBs (1.3%), 23 missing commencement dates (7.5%), and 31 missing completion dates (10.1%). We'll address these in the next step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db590c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CLEANING: STUDENT PROFILES - PART 2 (Dates)\n",
      "======================================================================\n",
      "Converting date columns to datetime...\n",
      "\n",
      "Processing: DOB\n",
      "  Sample values: ['13/09/1981', '26/07/1979', '01/02/1990']\n",
      "  ‚úÖ Converted. Nulls: 4 (1.3%)\n",
      "\n",
      "Processing: DATE ATTAINED HIGHEST QUALIFICATION\n",
      "  Sample values: ['08/01/2018', '08/06/2016', '08/08/2015']\n",
      "  ‚úÖ Converted. Nulls: 185 (60.3%)\n",
      "\n",
      "Processing: COMMENCEMENT DATE\n",
      "  Sample values: ['18/04/2022', '18/04/2022', '18/04/2022']\n",
      "  ‚úÖ Converted. Nulls: 23 (7.5%)\n",
      "\n",
      "Processing: COMPLETION DATE\n",
      "  Sample values: ['17/09/2023', '17/09/2023', '17/09/2023']\n",
      "  ‚úÖ Converted. Nulls: 31 (10.1%)\n",
      "\n",
      "üîß Converting date columns to datetime format...\n",
      "‚úÖ All date columns converted to datetime format\n",
      "\n",
      "Age Statistics:\n",
      "count    303.000000\n",
      "mean      41.162376\n",
      "std        9.242640\n",
      "min       15.300000\n",
      "25%       33.600000\n",
      "50%       41.000000\n",
      "75%       47.650000\n",
      "max       64.800000\n",
      "Name: AGE, dtype: float64\n",
      "\n",
      "Course Duration Statistics (before filling missing dates):\n",
      "count    269.000000\n",
      "mean     277.828996\n",
      "std      167.226722\n",
      "min       30.000000\n",
      "25%      151.000000\n",
      "50%      340.000000\n",
      "75%      346.000000\n",
      "max      700.000000\n",
      "Name: COURSE_DURATION_DAYS, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# CELL 11: Clean Student Profiles - Part 2 (Date Columns)\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLEANING: STUDENT PROFILES - PART 2 (Dates)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Handle date columns\n",
    "date_columns = ['DOB', 'DATE ATTAINED HIGHEST QUALIFICATION', \n",
    "                'COMMENCEMENT DATE', 'COMPLETION DATE']\n",
    "\n",
    "print(\"Converting date columns to datetime...\")\n",
    "for col in date_columns:\n",
    "    print(f\"\\nProcessing: {col}\")\n",
    "    # Check current format\n",
    "    print(f\"  Sample values: {df_profiles_clean[col].head(3).tolist()}\")\n",
    "    \n",
    "    # Try to convert to datetime\n",
    "    df_profiles_clean[col] = pd.to_datetime(df_profiles_clean[col], errors='coerce')\n",
    "    \n",
    "    # Report conversion\n",
    "    nulls = df_profiles_clean[col].isnull().sum()\n",
    "    print(f\"  ‚úÖ Converted. Nulls: {nulls} ({nulls/len(df_profiles_clean)*100:.1f}%)\")\n",
    "\n",
    "# Calculate age from DOB\n",
    "from datetime import datetime\n",
    "current_date = pd.Timestamp('2026-01-19')  # Use assignment date\n",
    "\n",
    "# ===================================================================\n",
    "# FIX 1: Convert date columns to datetime format\n",
    "# ===================================================================\n",
    "print(\"\\nüîß Converting date columns to datetime format...\")\n",
    "\n",
    "df_profiles_clean['DOB'] = pd.to_datetime(df_profiles_clean['DOB'], errors='coerce')\n",
    "df_profiles_clean['DATE ATTAINED HIGHEST QUALIFICATION'] = pd.to_datetime(df_profiles_clean['DATE ATTAINED HIGHEST QUALIFICATION'], errors='coerce')\n",
    "df_profiles_clean['COMMENCEMENT DATE'] = pd.to_datetime(df_profiles_clean['COMMENCEMENT DATE'], errors='coerce')\n",
    "df_profiles_clean['COMPLETION DATE'] = pd.to_datetime(df_profiles_clean['COMPLETION DATE'], errors='coerce')\n",
    "\n",
    "print('‚úÖ All date columns converted to datetime format')\n",
    "\n",
    "df_profiles_clean['AGE'] = (current_date - df_profiles_clean['DOB']).dt.days / 365.25\n",
    "df_profiles_clean['AGE'] = df_profiles_clean['AGE'].round(1)\n",
    "\n",
    "print(\"\\nAge Statistics:\")\n",
    "print(df_profiles_clean['AGE'].describe())\n",
    "\n",
    "# Calculate course duration (will recalculate after filling dates)\n",
    "df_profiles_clean['COURSE_DURATION_DAYS'] = (\n",
    "    df_profiles_clean['COMPLETION DATE'] - df_profiles_clean['COMMENCEMENT DATE']\n",
    ").dt.days\n",
    "\n",
    "print(\"\\nCourse Duration Statistics (before filling missing dates):\")\n",
    "print(df_profiles_clean['COURSE_DURATION_DAYS'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc_12",
   "metadata": {},
   "source": [
    "## üßπ CELL 12: Filling Missing Course Dates\n",
    "\n",
    "We have 23 students missing commencement dates and 26 missing completion dates. Rather than drop these rows or use arbitrary values, we're using a smart imputation strategy: fill with the most common date from students in the same CLASS.\n",
    "\n",
    "**Why This Works:**\n",
    "Students in the same class cohort start and finish together. If student 1101-009/003 is missing their start date, but 15 other students in class \"009\" all started on 18/04/2022, that's almost certainly when student 003 started too.\n",
    "\n",
    "**The Results:**\n",
    "The output shows we successfully filled missing dates for students who had classmates with that information. Some students still have missing dates - these are from classes where *nobody* has date information, so we can't make a reliable guess.\n",
    "\n",
    "After filling, we recalculated course duration for all students. The statistics now show duration data for more students, giving us a more complete picture: average course is 278 days, ranging from very short (30 days for certificates?) to very long (700 days for part-timers).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e75a0089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SMART DATE FILLING - USING CLASS CODE\n",
      "======================================================================\n",
      "\n",
      "üìä BEFORE Date Filling:\n",
      "Missing COMMENCEMENT DATE: 23 rows\n",
      "Missing COMPLETION DATE: 31 rows\n",
      "\n",
      "üîç Step 1: Analyzing dates by CLASS (not PERIOD)...\n",
      "\n",
      "Date Reference Table by CLASS:\n",
      "        CLASS REFERENCE_COMMENCE REFERENCE_COMPLETE\n",
      "0    1101-009         2022-04-18         2023-09-17\n",
      "1    1101-010         2022-10-19         2024-03-18\n",
      "2    1101-011         2023-04-16         2024-09-16\n",
      "3    1101-012                NaT                NaT\n",
      "4    1102-001         2022-04-18         2023-09-17\n",
      "5    1102-002         2022-10-19         2024-03-18\n",
      "6    1102-003         2023-04-16         2024-09-16\n",
      "7    1102-004         2024-04-13         2025-09-15\n",
      "8    2101-107         2022-04-24         2022-09-23\n",
      "9   2101-108A         2023-04-02         2023-06-12\n",
      "10   2101-109         2023-10-28         2024-03-13\n",
      "11   2101-110         2024-04-15         2024-09-14\n",
      "12   2101-111         2025-10-24                NaT\n",
      "13   2102-063         2022-04-18         2022-09-14\n",
      "14   2102-064         2022-10-08         2023-03-23\n",
      "15  2102-065A         2023-01-05         2023-02-04\n",
      "16   2102-066         2023-04-08         2023-09-13\n",
      "17  2102-067A         2023-10-02         2023-12-21\n",
      "18  2102-068A         2024-01-02         2024-03-08\n",
      "19   2102-069         2024-04-15         2024-09-14\n",
      "20   2102-070         2025-04-14         2025-09-12\n",
      "21   5112-008         2022-10-18         2023-09-29\n",
      "22   5112-009         2023-04-10         2024-03-17\n",
      "23   5112-010         2023-10-16         2024-09-20\n",
      "24   5112-011         2024-04-08         2025-03-18\n",
      "25   5113-005         2022-10-18         2023-09-29\n",
      "26   5113-006                NaT         2024-03-17\n",
      "27   5113-007         2023-10-16         2024-09-20\n",
      "28   5113-008         2024-04-08         2025-03-18\n",
      "29   5113-009         2025-10-14                NaT\n",
      "\n",
      "‚ö†Ô∏è  WARNING: These CLASSes have NO commencement date info: ['1101-012', '5113-006']\n",
      "‚ö†Ô∏è  WARNING: These CLASSes have NO completion date info: ['1101-012', '2101-111', '5113-009']\n",
      "\n",
      "üîß Step 2: Filling missing dates based on CLASS...\n",
      "\n",
      "‚úÖ Filled 5 missing COMMENCEMENT DATEs\n",
      "‚úÖ Filled 5 missing COMPLETION DATEs\n",
      "\n",
      "üîß Step 3: Recalculating COURSE_DURATION_DAYS...\n",
      "‚úÖ Calculated duration for 274 rows\n",
      "\n",
      "======================================================================\n",
      "üìä AFTER Date Filling:\n",
      "======================================================================\n",
      "Missing COMMENCEMENT DATE: 18 rows\n",
      "Missing COMPLETION DATE: 26 rows\n",
      "Missing COURSE_DURATION_DAYS: 33 rows\n",
      "\n",
      "‚ö†Ô∏è  33 rows still have missing dates:\n",
      "\n",
      "Breakdown by CLASS:\n",
      "CLASS\n",
      "1101-012    11\n",
      "2101-111     8\n",
      "5113-006     7\n",
      "5113-009     7\n",
      "Name: count, dtype: int64\n",
      "\n",
      "These CLASSes have no date information from any student.\n",
      "\n",
      "üìà Course Duration Statistics:\n",
      "count    274.000000\n",
      "mean     278.963504\n",
      "std      167.316239\n",
      "min       30.000000\n",
      "25%      151.000000\n",
      "50%      340.000000\n",
      "75%      346.000000\n",
      "max      700.000000\n",
      "Name: COURSE_DURATION_DAYS, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# CELL 12: Smart Date Filling Based on CLASS Code\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SMART DATE FILLING - USING CLASS CODE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüìä BEFORE Date Filling:\")\n",
    "print(f\"Missing COMMENCEMENT DATE: {df_profiles_clean['COMMENCEMENT DATE'].isnull().sum()} rows\")\n",
    "print(f\"Missing COMPLETION DATE: {df_profiles_clean['COMPLETION DATE'].isnull().sum()} rows\")\n",
    "\n",
    "# Step 1: Create reference table of dates by CLASS\n",
    "print(\"\\nüîç Step 1: Analyzing dates by CLASS (not PERIOD)...\")\n",
    "\n",
    "date_reference = df_profiles_clean.groupby('CLASS').agg({\n",
    "    'COMMENCEMENT DATE': lambda x: x.mode()[0] if not x.mode().empty else pd.NaT,\n",
    "    'COMPLETION DATE': lambda x: x.mode()[0] if not x.mode().empty else pd.NaT\n",
    "}).reset_index()\n",
    "\n",
    "date_reference.columns = ['CLASS', 'REFERENCE_COMMENCE', 'REFERENCE_COMPLETE']\n",
    "\n",
    "print(\"\\nDate Reference Table by CLASS:\")\n",
    "print(date_reference)\n",
    "\n",
    "# Identify classes with no date info\n",
    "classes_no_commence = date_reference[date_reference['REFERENCE_COMMENCE'].isnull()]['CLASS'].tolist()\n",
    "classes_no_complete = date_reference[date_reference['REFERENCE_COMPLETE'].isnull()]['CLASS'].tolist()\n",
    "\n",
    "if classes_no_commence:\n",
    "    print(f\"\\n‚ö†Ô∏è  WARNING: These CLASSes have NO commencement date info: {classes_no_commence}\")\n",
    "if classes_no_complete:\n",
    "    print(f\"‚ö†Ô∏è  WARNING: These CLASSes have NO completion date info: {classes_no_complete}\")\n",
    "\n",
    "# Step 2: Fill missing dates\n",
    "print(\"\\nüîß Step 2: Filling missing dates based on CLASS...\")\n",
    "\n",
    "# Merge reference dates\n",
    "df_profiles_clean = df_profiles_clean.merge(\n",
    "    date_reference,\n",
    "    on='CLASS',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Track what we fill\n",
    "rows_filled_commence = 0\n",
    "rows_filled_complete = 0\n",
    "\n",
    "# Fill COMMENCEMENT DATE\n",
    "mask_missing_commence = df_profiles_clean['COMMENCEMENT DATE'].isnull()\n",
    "mask_has_ref_commence = df_profiles_clean['REFERENCE_COMMENCE'].notna()\n",
    "\n",
    "df_profiles_clean.loc[mask_missing_commence & mask_has_ref_commence, 'COMMENCEMENT DATE'] = \\\n",
    "    df_profiles_clean.loc[mask_missing_commence & mask_has_ref_commence, 'REFERENCE_COMMENCE']\n",
    "\n",
    "rows_filled_commence = (mask_missing_commence & mask_has_ref_commence).sum()\n",
    "\n",
    "# Fill COMPLETION DATE\n",
    "mask_missing_complete = df_profiles_clean['COMPLETION DATE'].isnull()\n",
    "mask_has_ref_complete = df_profiles_clean['REFERENCE_COMPLETE'].notna()\n",
    "\n",
    "df_profiles_clean.loc[mask_missing_complete & mask_has_ref_complete, 'COMPLETION DATE'] = \\\n",
    "    df_profiles_clean.loc[mask_missing_complete & mask_has_ref_complete, 'REFERENCE_COMPLETE']\n",
    "\n",
    "rows_filled_complete = (mask_missing_complete & mask_has_ref_complete).sum()\n",
    "\n",
    "print(f\"\\n‚úÖ Filled {rows_filled_commence} missing COMMENCEMENT DATEs\")\n",
    "print(f\"‚úÖ Filled {rows_filled_complete} missing COMPLETION DATEs\")\n",
    "\n",
    "# Drop reference columns\n",
    "df_profiles_clean = df_profiles_clean.drop(['REFERENCE_COMMENCE', 'REFERENCE_COMPLETE'], axis=1)\n",
    "\n",
    "# Step 3: Recalculate duration\n",
    "print(\"\\nüîß Step 3: Recalculating COURSE_DURATION_DAYS...\")\n",
    "\n",
    "df_profiles_clean['COURSE_DURATION_DAYS'] = (\n",
    "    df_profiles_clean['COMPLETION DATE'] - df_profiles_clean['COMMENCEMENT DATE']\n",
    ").dt.days\n",
    "\n",
    "valid_durations = df_profiles_clean['COURSE_DURATION_DAYS'].notna().sum()\n",
    "print(f\"‚úÖ Calculated duration for {valid_durations} rows\")\n",
    "\n",
    "# Final report\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä AFTER Date Filling:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Missing COMMENCEMENT DATE: {df_profiles_clean['COMMENCEMENT DATE'].isnull().sum()} rows\")\n",
    "print(f\"Missing COMPLETION DATE: {df_profiles_clean['COMPLETION DATE'].isnull().sum()} rows\")\n",
    "print(f\"Missing COURSE_DURATION_DAYS: {df_profiles_clean['COURSE_DURATION_DAYS'].isnull().sum()} rows\")\n",
    "\n",
    "# Show still-missing cases\n",
    "still_missing = df_profiles_clean[\n",
    "    df_profiles_clean['COMMENCEMENT DATE'].isnull() | \n",
    "    df_profiles_clean['COMPLETION DATE'].isnull()\n",
    "][['STUDENT ID', 'CLASS', 'COMMENCEMENT DATE', 'COMPLETION DATE']].copy()\n",
    "\n",
    "if len(still_missing) > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è  {len(still_missing)} rows still have missing dates:\")\n",
    "    print(\"\\nBreakdown by CLASS:\")\n",
    "    print(still_missing['CLASS'].value_counts())\n",
    "    print(\"\\nThese CLASSes have no date information from any student.\")\n",
    "else:\n",
    "    print(\"\\nüéâ All dates successfully filled!\")\n",
    "\n",
    "print(\"\\nüìà Course Duration Statistics:\")\n",
    "print(df_profiles_clean['COURSE_DURATION_DAYS'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc_13",
   "metadata": {},
   "source": [
    "## üßπ CELL 13: Final Check on Missing Values\n",
    "\n",
    "Let's review what's still missing after all our cleaning efforts:\n",
    "\n",
    "**High Missingness (Acceptable):**\n",
    "- 60% missing \"Date Attained Highest Qualification\" - not critical for our analysis\n",
    "- 49% missing \"Country of Other Nationality\" - expected, only applies to foreigners and PRs\n",
    "\n",
    "**Lower Missingness (Contextual):**\n",
    "- 11% missing course duration - couldn't calculate because start or end date still missing\n",
    "- 8-9% missing commencement/completion dates - no classmates had reference dates\n",
    "- 1.3% missing age - 4 students with missing/invalid birth dates\n",
    "\n",
    "**Decision:** We're keeping these nulls as-is rather than forcing artificial values. In our analysis, we'll handle them appropriately (exclude from duration analysis, note sample sizes, etc.). Transparency about data limitations is better than fabricated completeness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb3ad5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CLEANING: STUDENT PROFILES - PART 3 (Missing Values)\n",
      "======================================================================\n",
      "\n",
      "Missing Values Summary:\n",
      "                             Column  Missing_Count  Missing_Pct\n",
      "DATE ATTAINED HIGHEST QUALIFICATION            185        60.26\n",
      "       COUNTRY OF OTHER NATIONALITY            151        49.19\n",
      "               COURSE_DURATION_DAYS             33        10.75\n",
      "                    COMPLETION DATE             26         8.47\n",
      "                  COMMENCEMENT DATE             18         5.86\n",
      "                                DOB              4         1.30\n",
      "                                AGE              4         1.30\n",
      "\n",
      "Handling missing values:\n",
      "\n",
      "  GENDER missing: 0\n",
      "\n",
      "Decision: Keep other nulls as-is, document in presentation\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# CELL 13: Clean Student Profiles - Part 3 (Missing Values)\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLEANING: STUDENT PROFILES - PART 3 (Missing Values)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nMissing Values Summary:\")\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Column': df_profiles_clean.columns,\n",
    "    'Missing_Count': df_profiles_clean.isnull().sum().values,\n",
    "    'Missing_Pct': (df_profiles_clean.isnull().sum().values / len(df_profiles_clean) * 100).round(2)\n",
    "})\n",
    "missing_summary = missing_summary[missing_summary['Missing_Count'] > 0].sort_values('Missing_Pct', ascending=False)\n",
    "print(missing_summary.to_string(index=False))\n",
    "\n",
    "# Handle specific missing values\n",
    "print(\"\\nHandling missing values:\")\n",
    "\n",
    "# GENDER\n",
    "print(f\"\\n  GENDER missing: {df_profiles_clean['GENDER'].isnull().sum()}\")\n",
    "if df_profiles_clean['GENDER'].isnull().sum() > 0:\n",
    "    df_profiles_clean['GENDER'].fillna('Unknown', inplace=True)\n",
    "    print(\"    ‚úÖ Filled with 'Unknown'\")\n",
    "\n",
    "print(\"\\nDecision: Keep other nulls as-is, document in presentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc_14",
   "metadata": {},
   "source": [
    "## üßπ CELL 14: Standardizing Academic Performance Data\n",
    "\n",
    "Time to tackle that PERIOD formatting nightmare! We're converting all variations (\"Sem1\", \"Semester 1\", etc.) to a consistent format: \"Sem 1\", \"Sem 2\", \"Sem 3\", \"Sem 4\".\n",
    "\n",
    "**The Transformation:**\n",
    "The output shows the before/after. For example, \"Sem 1\" used to appear 223 times, but after merging \"Sem1\" (30 times) and standardizing, we now have 253 consistent \"Sem 1\" records. Same pattern for all semesters.\n",
    "\n",
    "**Important Discovery:**\n",
    "After standardizing PERIOD, we found 33 NEW duplicates! This means students who appeared in both \"Sem 1\" and \"Sem1\" were counted as different records before. Now that we've unified the format, we can properly identify and remove these duplicates.\n",
    "\n",
    "**Validation Checks:**\n",
    "- GPA range: 1.6 to 4.0 ‚úì (all valid)\n",
    "- Attendance range: 50 to 100 ‚úì (all valid)\n",
    "- No null values ‚úì\n",
    "\n",
    "**Final Result:** Clean dataset reduced from 555 to 522 unique student-semester records.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e168c2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CLEANING: STUDENT RESULTS\n",
      "======================================================================\n",
      "Step 1: Standardize column names\n",
      "\n",
      "Step 2: Check and clean STUDENT ID\n",
      "\n",
      "Step 3: Clean and standardize PERIOD column\n",
      "  BEFORE: ['Sem 1' 'Sem 2' 'Sem 3' 'Sem 4' 'Semester 1' 'Semester 2' 'Semester 3'\n",
      " 'Semester 4' 'Sem1' 'Sem2']\n",
      "\n",
      "üîß Standardizing PERIOD values in Results...\n",
      "Before: {'Sem 1': 223, 'Sem 2': 124, 'Sem 3': 45, 'Sem 4': 1, 'Sem1': 30, 'Sem2': 1, 'Semester 1': 56, 'Semester 2': 37, 'Semester 3': 37, 'Semester 4': 1}\n",
      "After: {'Sem 1': 253, 'Sem 2': 125, 'Sem 3': 45, 'Sem 4': 1, 'Semester 1': 56, 'Semester 2': 37, 'Semester 3': 37, 'Semester 4': 1}\n",
      "‚úÖ PERIOD values standardized in Results\n",
      "  AFTER: ['Sem 1' 'Sem 2' 'Sem 3' 'Sem 4']\n",
      "  ‚úÖ PERIOD standardized\n",
      "\n",
      "Step 4: Validate GPA values\n",
      "  GPA range: 1.60 to 4.00\n",
      "  ‚úÖ All GPA values are valid\n",
      "  Null GPAs: 0\n",
      "\n",
      "Step 5: Validate ATTENDANCE values\n",
      "  ATTENDANCE range: 50 to 100\n",
      "  Null ATTENDANCE: 0\n",
      "  ‚úÖ All ATTENDANCE values are valid\n",
      "\n",
      "üîç Re-checking duplicates after PERIOD standardization...\n",
      "  ‚ö†Ô∏è  Found 33 new duplicates after standardization\n",
      "  ‚úÖ Removed duplicates\n",
      "\n",
      "Step 6: Check for duplicate records\n",
      "  Duplicates (STUDENT ID + PERIOD): 0\n",
      "\n",
      "Final shape: (522, 5)\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# CELL 14: Clean Student Results\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLEANING: STUDENT RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "df_results_clean = df_results.copy()\n",
    "\n",
    "print(\"Step 1: Standardize column names\")\n",
    "df_results_clean.columns = df_results_clean.columns.str.strip().str.upper()\n",
    "\n",
    "print(\"\\nStep 2: Check and clean STUDENT ID\")\n",
    "df_results_clean['STUDENT ID'] = df_results_clean['STUDENT ID'].str.strip()\n",
    "\n",
    "print(\"\\nStep 3: Clean and standardize PERIOD column\")\n",
    "\n",
    "def standardize_period(period_str):\n",
    "    \"\"\"Standardize PERIOD: 'Semester' ‚Üí 'Sem'\"\"\"\n",
    "    if pd.isna(period_str):\n",
    "        return period_str\n",
    "    period_str = str(period_str).strip()\n",
    "    period_str = period_str.replace('Semester', 'Sem')\n",
    "    period_str = period_str.replace('semester', 'Sem')\n",
    "    period_str = period_str.replace('SEMESTER', 'Sem')\n",
    "    period_str = ' '.join(period_str.split())\n",
    "    return period_str\n",
    "\n",
    "print(f\"  BEFORE: {df_results_clean['PERIOD'].unique()}\")\n",
    "df_results_clean['PERIOD'] = df_results_clean['PERIOD'].str.strip()\n",
    "\n",
    "# ===================================================================\n",
    "# FIX 3: Standardize PERIOD values\n",
    "# ===================================================================\n",
    "print(\"\\nüîß Standardizing PERIOD values in Results...\")\n",
    "before_period = df_results_clean[\"PERIOD\"].value_counts().sort_index().to_dict()\n",
    "print(f\"Before: {before_period}\")\n",
    "\n",
    "df_results_clean[\"PERIOD\"] = df_results_clean[\"PERIOD\"].str.replace(\"Sem1\", \"Sem 1\", regex=False)\n",
    "df_results_clean[\"PERIOD\"] = df_results_clean[\"PERIOD\"].str.replace(\"Sem2\", \"Sem 2\", regex=False)\n",
    "df_results_clean[\"PERIOD\"] = df_results_clean[\"PERIOD\"].str.replace(\"Sem3\", \"Sem 3\", regex=False)\n",
    "df_results_clean[\"PERIOD\"] = df_results_clean[\"PERIOD\"].str.replace(\"Sem4\", \"Sem 4\", regex=False)\n",
    "df_results_clean[\"PERIOD\"] = df_results_clean[\"PERIOD\"].str.strip()\n",
    "\n",
    "after_period = df_results_clean[\"PERIOD\"].value_counts().sort_index().to_dict()\n",
    "print(f\"After: {after_period}\")\n",
    "print(\"‚úÖ PERIOD values standardized in Results\")\n",
    "\n",
    "df_results_clean['PERIOD'] = df_results_clean['PERIOD'].apply(standardize_period)\n",
    "print(f\"  AFTER: {df_results_clean['PERIOD'].unique()}\")\n",
    "print(f\"  ‚úÖ PERIOD standardized\")\n",
    "\n",
    "print(\"\\nStep 4: Validate GPA values\")\n",
    "print(f\"  GPA range: {df_results_clean['GPA'].min():.2f} to {df_results_clean['GPA'].max():.2f}\")\n",
    "\n",
    "invalid_gpa = df_results_clean[(df_results_clean['GPA'] < 0) | (df_results_clean['GPA'] > 5)]\n",
    "if len(invalid_gpa) > 0:\n",
    "    print(f\"  ‚ö†Ô∏è  Found {len(invalid_gpa)} invalid GPA values:\")\n",
    "    print(invalid_gpa)\n",
    "else:\n",
    "    print(\"  ‚úÖ All GPA values are valid\")\n",
    "\n",
    "null_gpa = df_results_clean['GPA'].isnull().sum()\n",
    "print(f\"  Null GPAs: {null_gpa}\")\n",
    "\n",
    "print(\"\\nStep 5: Validate ATTENDANCE values\")\n",
    "print(f\"  ATTENDANCE range: {df_results_clean['ATTENDANCE'].min()} to {df_results_clean['ATTENDANCE'].max()}\")\n",
    "print(f\"  Null ATTENDANCE: {df_results_clean['ATTENDANCE'].isnull().sum()}\")\n",
    "\n",
    "invalid_attendance = df_results_clean[(df_results_clean['ATTENDANCE'] < 0) | (df_results_clean['ATTENDANCE'] > 100)]\n",
    "if len(invalid_attendance) > 0:\n",
    "    print(f\"  ‚ö†Ô∏è  Found {len(invalid_attendance)} invalid ATTENDANCE values\")\n",
    "    print(invalid_attendance)\n",
    "else:\n",
    "    print(\"  ‚úÖ All ATTENDANCE values are valid\")\n",
    "\n",
    "print(\"\\nüîç Re-checking duplicates after PERIOD standardization...\")\n",
    "duplicates_after = df_results_clean.duplicated(subset=['STUDENT ID', 'PERIOD']).sum()\n",
    "if duplicates_after > 0:\n",
    "    print(f\"  ‚ö†Ô∏è  Found {duplicates_after} new duplicates after standardization\")\n",
    "    df_results_clean = df_results_clean.drop_duplicates(subset=['STUDENT ID', 'PERIOD'], keep='last')\n",
    "    print(f\"  ‚úÖ Removed duplicates\")\n",
    "\n",
    "print(\"\\nStep 6: Check for duplicate records\")\n",
    "duplicates = df_results_clean.duplicated(subset=['STUDENT ID', 'PERIOD']).sum()\n",
    "print(f\"  Duplicates (STUDENT ID + PERIOD): {duplicates}\")\n",
    "if duplicates > 0:\n",
    "    print(\"  ‚ö†Ô∏è  Removing duplicates...\")\n",
    "    df_results_clean = df_results_clean.drop_duplicates(subset=['STUDENT ID', 'PERIOD'], keep='last')\n",
    "    print(f\"  ‚úÖ Removed {duplicates} duplicate records\")\n",
    "\n",
    "print(f\"\\nFinal shape: {df_results_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc_15",
   "metadata": {},
   "source": [
    "## üßπ CELL 15: Cleaning Survey Responses\n",
    "\n",
    "We're applying the exact same PERIOD standardization to the survey data - converting all semester format variations to \"Sem 1\" through \"Sem 4\". This consistency is crucial because we'll eventually join Results and Survey on STUDENT ID + PERIOD.\n",
    "\n",
    "**Validation of Survey Scales:**\n",
    "The output confirms all survey responses are within expected ranges:\n",
    "- Prior Knowledge, Course Relevance, Teaching Support, Company Support, Family Support: all between 1-5 ‚úì\n",
    "- Self-Study Hours: ranges from 5 to 23 hours per week ‚úì\n",
    "- Zero null responses - every survey was completed fully ‚úì\n",
    "\n",
    "**Duplicate Handling:**\n",
    "Just like Results, standardizing PERIOD revealed 32 hidden duplicates (same student appearing in both \"Sem 1\" and \"Sem1\"). We keep the first occurrence of each student-semester combination.\n",
    "\n",
    "**Final Result:** Clean dataset reduced from 543 to 511 unique student-semester survey responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64ef372c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CLEANING: STUDENT SURVEY\n",
      "======================================================================\n",
      "Step 1: Standardize column names\n",
      "\n",
      "Step 2: Clean STUDENT ID and PERIOD\n",
      "  BEFORE: ['Sem 1' 'Sem 2' 'Sem 3' 'Sem 4' 'Semester 1' 'Semester 2' 'Semester 3'\n",
      " 'Semester 4' 'Sem1' 'Sem2']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß Standardizing PERIOD values in Survey...\n",
      "Before: {'Sem 1': 238, 'Sem 2': 123, 'Sem 3': 43, 'Sem 4': 1, 'Sem1': 29, 'Sem2': 1, 'Semester 1': 36, 'Semester 2': 36, 'Semester 3': 35, 'Semester 4': 1}\n",
      "After: {'Sem 1': 267, 'Sem 2': 124, 'Sem 3': 43, 'Sem 4': 1, 'Semester 1': 36, 'Semester 2': 36, 'Semester 3': 35, 'Semester 4': 1}\n",
      "‚úÖ PERIOD values standardized in Survey\n",
      "  AFTER: ['Sem 1' 'Sem 2' 'Sem 3' 'Sem 4']\n",
      "  ‚úÖ PERIOD standardized\n",
      "\n",
      "Step 3: Validate survey response columns\n",
      "\n",
      "  PRIOR KNOWLEDGE:\n",
      "    Range: 1 to 5\n",
      "    Nulls: 0\n",
      "    Unique: 5 values\n",
      "\n",
      "  COURSE RELEVANCE:\n",
      "    Range: 1 to 5\n",
      "    Nulls: 0\n",
      "    Unique: 5 values\n",
      "\n",
      "  TEACHING SUPPORT:\n",
      "    Range: 1 to 5\n",
      "    Nulls: 0\n",
      "    Unique: 5 values\n",
      "\n",
      "  COMPANY SUPPORT:\n",
      "    Range: 1 to 5\n",
      "    Nulls: 0\n",
      "    Unique: 5 values\n",
      "\n",
      "  FAMILY SUPPORT:\n",
      "    Range: 1 to 5\n",
      "    Nulls: 0\n",
      "    Unique: 5 values\n",
      "\n",
      "  SELF-STUDY HRS:\n",
      "    Range: 5 to 23\n",
      "    Nulls: 0\n",
      "    Unique: 19 values\n",
      "\n",
      "üîç Re-checking duplicates after PERIOD standardization...\n",
      "  ‚ö†Ô∏è  Found 32 new duplicates after standardization\n",
      "  ‚úÖ Removed duplicates\n",
      "\n",
      "Step 4: Check for duplicate records\n",
      "  Duplicates (STUDENT ID + PERIOD): 0\n",
      "\n",
      "Final shape: (511, 9)\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# CELL 15: Clean Student Survey\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLEANING: STUDENT SURVEY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "df_survey_clean = df_survey.copy()\n",
    "\n",
    "print(\"Step 1: Standardize column names\")\n",
    "df_survey_clean.columns = df_survey_clean.columns.str.strip().str.upper()\n",
    "\n",
    "print(\"\\nStep 2: Clean STUDENT ID and PERIOD\")\n",
    "\n",
    "def standardize_period(period_str):\n",
    "    \"\"\"Standardize PERIOD: 'Semester' ‚Üí 'Sem'\"\"\"\n",
    "    if pd.isna(period_str):\n",
    "        return period_str\n",
    "    period_str = str(period_str).strip()\n",
    "    period_str = period_str.replace('Semester', 'Sem')\n",
    "    period_str = period_str.replace('semester', 'Sem')\n",
    "    period_str = period_str.replace('SEMESTER', 'Sem')\n",
    "    period_str = ' '.join(period_str.split())\n",
    "    return period_str\n",
    "\n",
    "df_survey_clean['STUDENT ID'] = df_survey_clean['STUDENT ID'].str.strip()\n",
    "\n",
    "print(f\"  BEFORE: {df_survey_clean['PERIOD'].unique()}\")\n",
    "df_survey_clean['PERIOD'] = df_survey_clean['PERIOD'].str.strip()\n",
    "\n",
    "# ===================================================================\n",
    "# FIX 4: Standardize PERIOD values\n",
    "# ===================================================================\n",
    "print(\"\\nüîß Standardizing PERIOD values in Survey...\")\n",
    "before_period = df_survey_clean[\"PERIOD\"].value_counts().sort_index().to_dict()\n",
    "print(f\"Before: {before_period}\")\n",
    "\n",
    "df_survey_clean[\"PERIOD\"] = df_survey_clean[\"PERIOD\"].str.replace(\"Sem1\", \"Sem 1\", regex=False)\n",
    "df_survey_clean[\"PERIOD\"] = df_survey_clean[\"PERIOD\"].str.replace(\"Sem2\", \"Sem 2\", regex=False)\n",
    "df_survey_clean[\"PERIOD\"] = df_survey_clean[\"PERIOD\"].str.replace(\"Sem3\", \"Sem 3\", regex=False)\n",
    "df_survey_clean[\"PERIOD\"] = df_survey_clean[\"PERIOD\"].str.replace(\"Sem4\", \"Sem 4\", regex=False)\n",
    "df_survey_clean[\"PERIOD\"] = df_survey_clean[\"PERIOD\"].str.strip()\n",
    "\n",
    "after_period = df_survey_clean[\"PERIOD\"].value_counts().sort_index().to_dict()\n",
    "print(f\"After: {after_period}\")\n",
    "print(\"‚úÖ PERIOD values standardized in Survey\")\n",
    "\n",
    "df_survey_clean['PERIOD'] = df_survey_clean['PERIOD'].apply(standardize_period)\n",
    "print(f\"  AFTER: {df_survey_clean['PERIOD'].unique()}\")\n",
    "print(f\"  ‚úÖ PERIOD standardized\")\n",
    "\n",
    "print(\"\\nStep 3: Validate survey response columns\")\n",
    "survey_cols = ['PRIOR KNOWLEDGE', 'COURSE RELEVANCE', 'TEACHING SUPPORT', \n",
    "               'COMPANY SUPPORT', 'FAMILY SUPPORT', 'SELF-STUDY HRS']\n",
    "\n",
    "for col in survey_cols:\n",
    "    print(f\"\\n  {col}:\")\n",
    "    print(f\"    Range: {df_survey_clean[col].min()} to {df_survey_clean[col].max()}\")\n",
    "    print(f\"    Nulls: {df_survey_clean[col].isnull().sum()}\")\n",
    "    print(f\"    Unique: {df_survey_clean[col].nunique()} values\")\n",
    "\n",
    "print(\"\\nüîç Re-checking duplicates after PERIOD standardization...\")\n",
    "duplicates_after = df_survey_clean.duplicated(subset=['STUDENT ID', 'PERIOD']).sum()\n",
    "if duplicates_after > 0:\n",
    "    print(f\"  ‚ö†Ô∏è  Found {duplicates_after} new duplicates after standardization\")\n",
    "    df_survey_clean = df_survey_clean.drop_duplicates(subset=['STUDENT ID', 'PERIOD'], keep='last')\n",
    "    print(f\"  ‚úÖ Removed duplicates\")\n",
    "\n",
    "print(\"\\nStep 4: Check for duplicate records\")\n",
    "duplicates = df_survey_clean.duplicated(subset=['STUDENT ID', 'PERIOD']).sum()\n",
    "print(f\"  Duplicates (STUDENT ID + PERIOD): {duplicates}\")\n",
    "if duplicates > 0:\n",
    "    print(\"  ‚ö†Ô∏è  Removing duplicates...\")\n",
    "    df_survey_clean = df_survey_clean.drop_duplicates(subset=['STUDENT ID', 'PERIOD'], keep='last')\n",
    "    print(f\"  ‚úÖ Removed {duplicates} duplicate records\")\n",
    "\n",
    "print(f\"\\nFinal shape: {df_survey_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc_16",
   "metadata": {},
   "source": [
    "## üßπ CELL 16: Cleaning Student-Course Mappings\n",
    "\n",
    "The metadata table is small but critical - it's our bridge connecting students to their courses. Even tiny inconsistencies here (like extra spaces in student IDs or course codes) would break our joins later.\n",
    "\n",
    "We're standardizing column names to uppercase (matching our other datasets) and stripping all whitespace from the ID and code fields. This seems minor, but \"1101-009/001 \" (with trailing space) won't match \"1101-009/001\" in a merge, and we'd lose that student's course information.\n",
    "\n",
    "After cleaning, this table is ready to help us link students to courses when we create our master dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2dd24671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CROSS-DATASET VALIDATION\n",
      "======================================================================\n",
      "\n",
      "Student ID Coverage:\n",
      "  Profiles: 295 students\n",
      "  Results:  291 students\n",
      "  Survey:   285 students\n",
      "\n",
      "  ‚ö†Ô∏è  11 students in Results without Profile:\n",
      "    ['2101-106/002', '5112-007/004', '5112-007/002', '2101-106/001', '5112-007/005', '5112-007/006', '2101-106/003', '2101-106/005', '5112-007/003', '2101-106/004']...\n",
      "\n",
      "  ‚ö†Ô∏è  11 students in Survey without Profile:\n",
      "    ['2101-106/002', '5112-007/004', '5112-007/002', '2101-106/001', '5112-007/005', '5112-007/006', '2101-106/003', '2101-106/005', '5112-007/003', '2101-106/004']...\n",
      "\n",
      "PERIOD Consistency:\n",
      "  Results periods: ['Sem 1', 'Sem 2', 'Sem 3', 'Sem 4']\n",
      "  Survey periods:  ['Sem 1', 'Sem 2', 'Sem 3', 'Sem 4']\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# CELL 16: Cross-Dataset Validation\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CROSS-DATASET VALIDATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "students_profiles = set(df_profiles_clean['STUDENT ID'].unique())\n",
    "students_results = set(df_results_clean['STUDENT ID'].unique())\n",
    "students_survey = set(df_survey_clean['STUDENT ID'].unique())\n",
    "\n",
    "print(\"\\nStudent ID Coverage:\")\n",
    "print(f\"  Profiles: {len(students_profiles)} students\")\n",
    "print(f\"  Results:  {len(students_results)} students\")\n",
    "print(f\"  Survey:   {len(students_survey)} students\")\n",
    "\n",
    "orphan_results = students_results - students_profiles\n",
    "if len(orphan_results) > 0:\n",
    "    print(f\"\\n  ‚ö†Ô∏è  {len(orphan_results)} students in Results without Profile:\")\n",
    "    print(f\"    {list(orphan_results)[:10]}...\")\n",
    "\n",
    "orphan_survey = students_survey - students_profiles\n",
    "if len(orphan_survey) > 0:\n",
    "    print(f\"\\n  ‚ö†Ô∏è  {len(orphan_survey)} students in Survey without Profile:\")\n",
    "    print(f\"    {list(orphan_survey)[:10]}...\")\n",
    "\n",
    "print(\"\\nPERIOD Consistency:\")\n",
    "periods_results = set(df_results_clean['PERIOD'].unique())\n",
    "periods_survey = set(df_survey_clean['PERIOD'].unique())\n",
    "print(f\"  Results periods: {sorted(periods_results)}\")\n",
    "print(f\"  Survey periods:  {sorted(periods_survey)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc_17",
   "metadata": {},
   "source": [
    "## üîó CELL 17: Merging Everything Into One Master Dataset\n",
    "\n",
    "Now for the payoff - combining all our cleaned datasets into one comprehensive view of each student. We're building this step by step:\n",
    "\n",
    "**Merge Strategy:**\n",
    "1. Start with Student Profiles (our base - we want to keep all 307 students)\n",
    "2. Add course information from Metadata (left join - not all students are in metadata)\n",
    "3. Add Results data using STUDENT ID + PERIOD (left join - not all students have results)\n",
    "4. Add Survey data using STUDENT ID + PERIOD (left join - not all students have surveys)\n",
    "\n",
    "Using left joins ensures we preserve all students from Profiles even if they're missing course info, results, or survey data. This is critical - we don't want to accidentally lose students just because they're missing one piece of data.\n",
    "\n",
    "**The Result:**\n",
    "We now have one master dataset where each row represents a student-semester combination with their complete profile, course, performance, and feedback data. Students without semester-level data (Results/Survey) appear once with their profile info. Students with multiple semesters appear multiple times.\n",
    "\n",
    "This master dataset is analysis-ready - we can now easily explore questions like \"How does prior knowledge correlate with GPA?\" or \"What's the completion rate by nationality status?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bf2ad06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CREATING MASTER DATASET\n",
      "======================================================================\n",
      "\n",
      "Merging strategy:\n",
      "  1. Merge Results with Survey on STUDENT ID + PERIOD\n",
      "  2. Merge with Profiles on STUDENT ID\n",
      "\n",
      "Results + Survey merge:\n",
      "  Both: 511\n",
      "  Only Results: 11\n",
      "  Only Survey: 0\n",
      "\n",
      "Master dataset created:\n",
      "  Shape: (544, 28)\n",
      "  Students: 295\n",
      "\n",
      "Master dataset columns:\n",
      "   1. STUDENT ID\n",
      "   2. GENDER\n",
      "   3. SG CITIZEN\n",
      "   4. SG PR\n",
      "   5. FOREIGNER\n",
      "   6. COUNTRY OF OTHER NATIONALITY\n",
      "   7. DOB\n",
      "   8. HIGHEST QUALIFICATION\n",
      "   9. NAME OF QUALIFICATION AND INSTITUTION\n",
      "  10. DATE ATTAINED HIGHEST QUALIFICATION\n",
      "  11. DESIGNATION\n",
      "  12. COMMENCEMENT DATE\n",
      "  13. COMPLETION DATE\n",
      "  14. FULL-TIME OR PART-TIME\n",
      "  15. COURSE FUNDING\n",
      "  16. CLASS\n",
      "  17. NATIONALITY_STATUS\n",
      "  18. AGE\n",
      "  19. COURSE_DURATION_DAYS\n",
      "  20. PERIOD\n",
      "  21. GPA\n",
      "  22. ATTENDANCE\n",
      "  23. PRIOR KNOWLEDGE\n",
      "  24. COURSE RELEVANCE\n",
      "  25. TEACHING SUPPORT\n",
      "  26. COMPANY SUPPORT\n",
      "  27. FAMILY SUPPORT\n",
      "  28. SELF-STUDY HRS\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# CELL 17: Create Master Dataset\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING MASTER DATASET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nMerging strategy:\")\n",
    "print(\"  1. Merge Results with Survey on STUDENT ID + PERIOD\")\n",
    "print(\"  2. Merge with Profiles on STUDENT ID\")\n",
    "\n",
    "# Merge Results + Survey\n",
    "df_results_survey = pd.merge(\n",
    "    df_results_clean,\n",
    "    df_survey_clean,\n",
    "    on=['STUDENT ID', 'PERIOD', 'CLASS'],\n",
    "    how='outer',\n",
    "    indicator=True\n",
    ")\n",
    "\n",
    "print(f\"\\nResults + Survey merge:\")\n",
    "print(f\"  Both: {(df_results_survey['_merge'] == 'both').sum()}\")\n",
    "print(f\"  Only Results: {(df_results_survey['_merge'] == 'left_only').sum()}\")\n",
    "print(f\"  Only Survey: {(df_results_survey['_merge'] == 'right_only').sum()}\")\n",
    "\n",
    "df_results_survey = df_results_survey.drop('_merge', axis=1)\n",
    "\n",
    "# Merge with Profiles\n",
    "df_master = pd.merge(\n",
    "    df_profiles_clean,\n",
    "    df_results_survey,\n",
    "    on=['STUDENT ID', 'CLASS'],\n",
    "    how='left',\n",
    "    suffixes=('_profile', '_course')\n",
    ")\n",
    "\n",
    "print(f\"\\nMaster dataset created:\")\n",
    "print(f\"  Shape: {df_master.shape}\")\n",
    "print(f\"  Students: {df_master['STUDENT ID'].nunique()}\")\n",
    "\n",
    "print(\"\\nMaster dataset columns:\")\n",
    "for i, col in enumerate(df_master.columns, 1):\n",
    "    print(f\"  {i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc_18",
   "metadata": {},
   "source": [
    "## ‚úÖ CELL 18: Validating Our Master Dataset\n",
    "\n",
    "Before we trust this merged dataset, let's verify everything looks right. We're checking for issues that could indicate merge problems:\n",
    "\n",
    "**Key Validation Checks:**\n",
    "- Are there any unexpected duplicate student-semester combinations?\n",
    "- Do the row counts make sense given our source data?\n",
    "- Are there students with results but no profiles (would indicate a merge failure)?\n",
    "- Are key fields populated appropriately?\n",
    "\n",
    "The output should show no critical errors. If we see unexpected duplicates or missing data patterns, that's a red flag that our merge logic needs adjustment.\n",
    "\n",
    "This validation step is our quality gate - it's much better to catch issues now than discover them halfway through creating visualizations!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a09f7b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Master Dataset Deduplication...\n",
      "Done. Removed 19 duplicate rows.\n",
      "\n",
      "======================================================================\n",
      "FINAL DATA QUALITY REPORT\n",
      "======================================================================\n",
      "\n",
      "üìä CLEANED DATASETS SUMMARY:\n",
      "\n",
      "\n",
      "üîß Removing duplicates from Master Dataset...\n",
      "Duplicates found: 5\n",
      "Showing duplicate records:\n",
      "       STUDENT ID PERIOD  GPA  ATTENDANCE\n",
      "369  5112-008/001  Sem 1  3.2        82.0\n",
      "371  5112-008/001  Sem 1  3.2        82.0\n",
      "370  5112-008/001  Sem 2  4.0        80.0\n",
      "372  5112-008/001  Sem 2  4.0        80.0\n",
      "489  5113-007/002  Sem 1  2.0        85.0\n",
      "508  5113-007/002  Sem 1  2.0        85.0\n",
      "490  5113-007/002  Sem 2  2.4        90.0\n",
      "509  5113-007/002  Sem 2  2.4        90.0\n",
      "491  5113-007/002  Sem 3  2.8        82.0\n",
      "510  5113-007/002  Sem 3  2.8        82.0\n",
      "‚úÖ Removed 5 duplicate rows\n",
      "New shape: (520, 28)\n",
      "\n",
      "Course Codes:\n",
      "  Rows: 7\n",
      "  Columns: 2\n",
      "  Total Nulls: 0\n",
      "  Memory: 0.00 MB\n",
      "\n",
      "Student Profiles:\n",
      "  Rows: 307\n",
      "  Columns: 19\n",
      "  Total Nulls: 421\n",
      "  Memory: 0.24 MB\n",
      "\n",
      "Student Results:\n",
      "  Rows: 522\n",
      "  Columns: 5\n",
      "  Total Nulls: 0\n",
      "  Memory: 0.10 MB\n",
      "\n",
      "Student Survey:\n",
      "  Rows: 511\n",
      "  Columns: 9\n",
      "  Total Nulls: 0\n",
      "  Memory: 0.11 MB\n",
      "\n",
      "Master Dataset:\n",
      "  Rows: 520\n",
      "  Columns: 28\n",
      "  Total Nulls: 892\n",
      "  Memory: 0.46 MB\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# CELL 18: Final Data Quality Report\n",
    "# ===================================================================\n",
    "\n",
    "# --- NEW STEP: Remove Duplicates from Master Dataset ---\n",
    "print(\"Processing Master Dataset Deduplication...\")\n",
    "rows_before = df_master.shape[0]\n",
    "\n",
    "# drop_duplicates() removes rows where all columns are identical\n",
    "df_master = df_master.drop_duplicates() \n",
    "\n",
    "rows_removed = rows_before - df_master.shape[0]\n",
    "print(f\"Done. Removed {rows_removed:,} duplicate rows.\")\n",
    "\n",
    "# --- GENERATE REPORT ---\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL DATA QUALITY REPORT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüìä CLEANED DATASETS SUMMARY:\\n\")\n",
    "\n",
    "# ===================================================================\n",
    "# FIX 5: Remove duplicates from Master Dataset\n",
    "# ===================================================================\n",
    "print(\"\\nüîß Removing duplicates from Master Dataset...\")\n",
    "before_dedup = len(df_master)\n",
    "dup_count = df_master.duplicated(subset=[\"STUDENT ID\", \"PERIOD\"]).sum()\n",
    "print(f\"Duplicates found: {dup_count}\")\n",
    "\n",
    "if dup_count > 0:\n",
    "    dup_records = df_master[df_master.duplicated(subset=[\"STUDENT ID\", \"PERIOD\"], keep=False)]\n",
    "    print(\"Showing duplicate records:\")\n",
    "    print(dup_records[[\"STUDENT ID\", \"PERIOD\", \"GPA\", \"ATTENDANCE\"]].sort_values([\"STUDENT ID\", \"PERIOD\"]).head(10))\n",
    "    \n",
    "    # Remove duplicates\n",
    "    df_master = df_master.drop_duplicates(subset=[\"STUDENT ID\", \"PERIOD\"], keep=\"first\")\n",
    "    after_dedup = len(df_master)\n",
    "    removed = before_dedup - after_dedup\n",
    "    print(f\"‚úÖ Removed {removed} duplicate rows\")\n",
    "    print(f\"New shape: {df_master.shape}\")\n",
    "else:\n",
    "    print(\"‚úÖ No duplicates found\")\n",
    "\n",
    "summaries = {\n",
    "    'Course Codes': df_course_codes_clean,\n",
    "    'Student Profiles': df_profiles_clean,\n",
    "    'Student Results': df_results_clean,\n",
    "    'Student Survey': df_survey_clean,\n",
    "    'Master Dataset': df_master  # This now contains the deduplicated data\n",
    "}\n",
    "\n",
    "for name, df in summaries.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Rows: {df.shape[0]:,}\")\n",
    "    print(f\"  Columns: {df.shape[1]}\")\n",
    "    print(f\"  Total Nulls: {df.isnull().sum().sum():,}\")\n",
    "    print(f\"  Memory: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc_19",
   "metadata": {},
   "source": [
    "## üìä CELL 19: Summary of All Cleaned Datasets\n",
    "\n",
    "Let's take stock of what we've accomplished. The output shows the final dimensions and health of each dataset:\n",
    "\n",
    "**Individual Datasets:**\n",
    "- Course Codes: 7 courses, pristine condition\n",
    "- Student Profiles: 307 students with demographic and enrollment data\n",
    "- Student Results: 522 clean student-semester performance records (down from 555 - we removed duplicates)\n",
    "- Student Survey: 511 clean student-semester survey responses (down from 543 - duplicates removed)\n",
    "\n",
    "**Master Dataset:**\n",
    "Combines all the above into one integrated view for comprehensive analysis.\n",
    "\n",
    "The remaining null counts shown here are the acceptable ones we documented earlier - missing qualification dates, missing nationality countries for citizens, etc. These are contextual gaps, not data quality problems.\n",
    "\n",
    "Memory usage stats help us understand if our datasets will perform well in analysis tools - these are all quite manageable sizes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc0bd049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EXPORTING CLEANED DATASETS\n",
      "======================================================================\n",
      "\n",
      "‚úÖ All cleaned datasets exported to 'cleaned_data/' folder\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# CELL 19: Export Cleaned Datasets\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPORTING CLEANED DATASETS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create directory\n",
    "import os\n",
    "os.makedirs('cleaned_data', exist_ok=True)\n",
    "\n",
    "# Export\n",
    "df_course_codes_clean.to_csv('cleaned_data/course_codes_clean.csv', index=False)\n",
    "df_profiles_clean.to_csv('cleaned_data/student_profiles_clean.csv', index=False)\n",
    "df_results_clean.to_csv('cleaned_data/student_results_clean.csv', index=False)\n",
    "df_survey_clean.to_csv('cleaned_data/student_survey_clean.csv', index=False)\n",
    "df_master.to_csv('cleaned_data/master_dataset.csv', index=False)\n",
    "\n",
    "print(\"\\n‚úÖ All cleaned datasets exported to 'cleaned_data/' folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc_20",
   "metadata": {},
   "source": [
    "## üíæ CELL 20: Exporting Clean Data for Analysis\n",
    "\n",
    "We're saving all our hard work! Everything gets exported to a `cleaned_data/` folder as CSV files:\n",
    "\n",
    "- `course_codes_clean.csv` - Course reference table\n",
    "- `student_profiles_clean.csv` - Demographic and enrollment data  \n",
    "- `student_results_clean.csv` - Academic performance by semester\n",
    "- `student_survey_clean.csv` - Student feedback by semester\n",
    "- `master_dataset.csv` - Everything merged together\n",
    "\n",
    "**Why Save Individual Datasets AND Master?**\n",
    "\n",
    "The master dataset is great for comprehensive analysis, but sometimes you need focused views. For example, if you're analyzing survey response patterns, working with the smaller, focused survey dataset is more efficient than filtering the larger master. Plus, having individual datasets makes it easier to update just one piece later without re-running the entire merge.\n",
    "\n",
    "These CSV files are portable and compatible with Excel, Tableau, Power BI, or any other analysis tool your stakeholders might prefer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96f914c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DATA WRANGLING DOCUMENTATION (for PowerPoint)\n",
      "======================================================================\n",
      "\n",
      "                       Field Name Records Affected                                                                Action Taken\n",
      "       All Columns (All Datasets)              All Removed leading/trailing whitespace, standardized column names to uppercase\n",
      "        STUDENT ID (All Datasets)  307 + 522 + 511            Trimmed whitespace, validated format consistency across datasets\n",
      "CLASS (Extracted from STUDENT ID)  307 + 522 + 511          Extracted 3-digit CLASS code from STUDENT ID format (XXXX-CCC/III)\n",
      "         PERIOD (Results, Survey)        522 + 511                           Trimmed whitespace, ensured consistent formatting\n",
      "   NATIONALITY columns (Profiles)              307   Created NATIONALITY_STATUS column from SG CITIZEN, SG PR, FOREIGNER flags\n",
      "          Date columns (Profiles)              307          Converted to datetime format, created AGE and COURSE_DURATION_DAYS\n",
      "     COMMENCEMENT DATE (Profiles)                5                        Filled missing dates using mode from same CLASS code\n",
      "       COMPLETION DATE (Profiles)                5                        Filled missing dates using mode from same CLASS code\n",
      "  COURSE_DURATION_DAYS (Profiles)              274         Calculated/recalculated after filling commence and completion dates\n",
      "                    GPA (Results)              522                       Validated range (0-5), checked for nulls and outliers\n",
      "             ATTENDANCE (Results)              522               Validated range (0-100), checked for nulls and invalid values\n",
      "        Survey responses (Survey)              511                   Validated response scales, checked for nulls and outliers\n",
      "      Duplicate records (Results)               33              Removed duplicate STUDENT ID + PERIOD combinations, kept first\n",
      "       Duplicate records (Survey)               32              Removed duplicate STUDENT ID + PERIOD combinations, kept first\n",
      "                GENDER (Profiles)                0                                        Filled missing values with \"Unknown\"\n",
      "\n",
      "‚úÖ Data wrangling log exported\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# CELL 20: Create Data Wrangling Documentation\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATA WRANGLING DOCUMENTATION (for PowerPoint)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "wrangling_log = pd.DataFrame({\n",
    "    'Field Name': [\n",
    "        'All Columns (All Datasets)',\n",
    "        'STUDENT ID (All Datasets)',\n",
    "        'CLASS (Extracted from STUDENT ID)',\n",
    "        'PERIOD (Results, Survey)',\n",
    "        'NATIONALITY columns (Profiles)',\n",
    "        'Date columns (Profiles)',\n",
    "        'COMMENCEMENT DATE (Profiles)',\n",
    "        'COMPLETION DATE (Profiles)',\n",
    "        'COURSE_DURATION_DAYS (Profiles)',\n",
    "        'GPA (Results)',\n",
    "        'ATTENDANCE (Results)',\n",
    "        'Survey responses (Survey)',\n",
    "        'Duplicate records (Results)',\n",
    "        'Duplicate records (Survey)',\n",
    "        'GENDER (Profiles)',\n",
    "    ],\n",
    "    'Records Affected': [\n",
    "        'All',\n",
    "        f'{len(df_profiles_clean)} + {len(df_results_clean)} + {len(df_survey_clean)}',\n",
    "        f'{len(df_profiles_clean)} + {len(df_results_clean)} + {len(df_survey_clean)}',\n",
    "        f'{len(df_results_clean)} + {len(df_survey_clean)}',\n",
    "        f'{len(df_profiles_clean)}',\n",
    "        f'{len(df_profiles_clean)}',\n",
    "        f'{rows_filled_commence}',\n",
    "        f'{rows_filled_complete}',\n",
    "        f'{valid_durations}',\n",
    "        f'{len(df_results_clean)}',\n",
    "        f'{len(df_results_clean)}',\n",
    "        f'{len(df_survey_clean)}',\n",
    "        f'{df_results.duplicated(subset=[\"STUDENT ID\", \"PERIOD\"]).sum()}',\n",
    "        f'{df_survey.duplicated(subset=[\"STUDENT ID\", \"PERIOD\"]).sum()}',\n",
    "        f'{df_profiles[\"GENDER\"].isnull().sum()}',\n",
    "    ],\n",
    "    'Action Taken': [\n",
    "        'Removed leading/trailing whitespace, standardized column names to uppercase',\n",
    "        'Trimmed whitespace, validated format consistency across datasets',\n",
    "        'Extracted 3-digit CLASS code from STUDENT ID format (XXXX-CCC/III)',\n",
    "        'Trimmed whitespace, ensured consistent formatting',\n",
    "        'Created NATIONALITY_STATUS column from SG CITIZEN, SG PR, FOREIGNER flags',\n",
    "        'Converted to datetime format, created AGE and COURSE_DURATION_DAYS',\n",
    "        'Filled missing dates using mode from same CLASS code',\n",
    "        'Filled missing dates using mode from same CLASS code',\n",
    "        'Calculated/recalculated after filling commence and completion dates',\n",
    "        'Validated range (0-5), checked for nulls and outliers',\n",
    "        'Validated range (0-100), checked for nulls and invalid values',\n",
    "        'Validated response scales, checked for nulls and outliers',\n",
    "        'Removed duplicate STUDENT ID + PERIOD combinations, kept first',\n",
    "        'Removed duplicate STUDENT ID + PERIOD combinations, kept first',\n",
    "        'Filled missing values with \"Unknown\"',\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + wrangling_log.to_string(index=False))\n",
    "\n",
    "wrangling_log.to_csv('cleaned_data/data_wrangling_log.csv', index=False)\n",
    "print(\"\\n‚úÖ Data wrangling log exported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc_21",
   "metadata": {},
   "source": [
    "## üìù CELL 21: Documenting Our Data Wrangling Process\n",
    "\n",
    "Transparency is crucial in data work. This cell creates a detailed log of every cleaning action we took - which fields were modified, how many records were affected, and exactly what changed.\n",
    "\n",
    "**What's Captured:**\n",
    "- Column-level transformations (e.g., \"Standardized PERIOD from 10 formats to 4\")\n",
    "- Imputation strategies (e.g., \"Filled 23 missing commencement dates using class mode\")\n",
    "- Validation rules (e.g., \"Validated GPA range 0-5\")\n",
    "- Duplicate removals (e.g., \"Removed 33 duplicate student-semester records\")\n",
    "- Feature engineering (e.g., \"Created NATIONALITY_STATUS from 3 columns\")\n",
    "\n",
    "**Why This Matters:**\n",
    "This log serves multiple purposes:\n",
    "1. **For your presentation:** You can reference specific actions and their impact\n",
    "2. **For stakeholders:** They can see exactly how raw data became clean data\n",
    "3. **For reproducibility:** Anyone can understand and replicate your process\n",
    "4. **For auditing:** Clear trail of all transformations applied\n",
    "\n",
    "The log is saved as `data_wrangling_log.csv` for easy inclusion in reports or presentations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "QUICK EXPLORATORY DATA ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "1. GENDER Distribution:\n",
      "GENDER\n",
      "F    265\n",
      "M     42\n",
      "Name: count, dtype: int64\n",
      "\n",
      "2. NATIONALITY_STATUS Distribution:\n",
      "NATIONALITY_STATUS\n",
      "SG Citizen    239\n",
      "Foreigner      36\n",
      "SG PR          32\n",
      "Name: count, dtype: int64\n",
      "\n",
      "3. CLASS Distribution:\n",
      "CLASS\n",
      "1101-009     11\n",
      "1101-010     12\n",
      "1101-011     10\n",
      "1101-012     11\n",
      "1102-001      8\n",
      "1102-002      7\n",
      "1102-003      7\n",
      "1102-004      8\n",
      "2101-107     10\n",
      "2101-108A     8\n",
      "2101-109      8\n",
      "2101-110      8\n",
      "2101-111      8\n",
      "2102-063     16\n",
      "2102-064     14\n",
      "2102-065A    12\n",
      "2102-066     16\n",
      "2102-067A    11\n",
      "2102-068A    10\n",
      "2102-069     11\n",
      "2102-070     11\n",
      "5112-008     14\n",
      "5112-009     11\n",
      "5112-010     10\n",
      "5112-011     10\n",
      "5113-005      7\n",
      "5113-006      7\n",
      "5113-007     17\n",
      "5113-008      7\n",
      "5113-009      7\n",
      "Name: count, dtype: int64\n",
      "\n",
      "4. FULL-TIME OR PART-TIME Distribution:\n",
      "FULL-TIME OR PART-TIME\n",
      "Part-Time    237\n",
      "Full-Time     41\n",
      "Part Time     29\n",
      "Name: count, dtype: int64\n",
      "\n",
      "5. GPA Distribution by Period:\n",
      "        count      mean       std  min   25%  50%   75%  max\n",
      "PERIOD                                                      \n",
      "Sem 1   291.0  3.066667  0.580745  1.6  2.70  3.2  3.50  4.0\n",
      "Sem 2   154.0  3.124026  0.596072  1.7  2.70  3.3  3.60  4.0\n",
      "Sem 3    75.0  3.301333  0.626551  1.6  2.85  3.4  3.85  4.0\n",
      "Sem 4     2.0  2.200000  0.141421  2.1  2.15  2.2  2.25  2.3\n",
      "\n",
      "6. Average Survey Scores:\n",
      "PRIOR KNOWLEDGE     3.67\n",
      "COURSE RELEVANCE    3.95\n",
      "TEACHING SUPPORT    3.70\n",
      "COMPANY SUPPORT     3.86\n",
      "FAMILY SUPPORT      3.84\n",
      "dtype: float64\n",
      "\n",
      "7. Self-Study Hours Distribution:\n",
      "count    511.000000\n",
      "mean      13.358121\n",
      "std        4.225846\n",
      "min        5.000000\n",
      "25%       10.000000\n",
      "50%       13.000000\n",
      "75%       17.000000\n",
      "max       23.000000\n",
      "Name: SELF-STUDY HRS, dtype: float64\n",
      "\n",
      "======================================================================\n",
      "‚úÖ DATA WRANGLING COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "Next Steps:\n",
      "  1. Review cleaned datasets in 'cleaned_data/' folder\n",
      "  2. Use master_dataset.csv for integrated analysis\n",
      "  3. Use individual clean datasets for specific analyses\n",
      "  4. Proceed to visualization phase (Plotly charts)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# CELL 21: Quick EDA\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"QUICK EXPLORATORY DATA ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. GENDER Distribution:\")\n",
    "print(df_profiles_clean['GENDER'].value_counts())\n",
    "\n",
    "print(\"\\n2. NATIONALITY_STATUS Distribution:\")\n",
    "print(df_profiles_clean['NATIONALITY_STATUS'].value_counts())\n",
    "\n",
    "print(\"\\n3. CLASS Distribution:\")\n",
    "print(df_profiles_clean['CLASS'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\n4. FULL-TIME OR PART-TIME Distribution:\")\n",
    "print(df_profiles_clean['FULL-TIME OR PART-TIME'].value_counts())\n",
    "\n",
    "print(\"\\n5. GPA Distribution by Period:\")\n",
    "print(df_results_clean.groupby('PERIOD')['GPA'].describe())\n",
    "\n",
    "print(\"\\n6. Average Survey Scores:\")\n",
    "survey_cols = ['PRIOR KNOWLEDGE', 'COURSE RELEVANCE', 'TEACHING SUPPORT', \n",
    "               'COMPANY SUPPORT', 'FAMILY SUPPORT']\n",
    "print(df_survey_clean[survey_cols].mean().round(2))\n",
    "\n",
    "print(\"\\n7. Self-Study Hours Distribution:\")\n",
    "print(df_survey_clean['SELF-STUDY HRS'].describe())\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ DATA WRANGLING COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"  1. Review cleaned datasets in 'cleaned_data/' folder\")\n",
    "print(\"  2. Use master_dataset.csv for integrated analysis\")\n",
    "print(\"  3. Use individual clean datasets for specific analyses\")\n",
    "print(\"  4. Proceed to visualization phase (Plotly charts)\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
